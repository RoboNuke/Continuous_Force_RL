# Hybrid Agent Control Experiment Configuration 

# Base configuration to inherit from
base_config: "configs/base/PiH_base.yaml"

# Experiment metadata
experiment:
  name: "explore_control"
  tags: []
  group: "explore_control"

wrappers:
  # Fragile object behavior
  fragile_objects:
    enabled: true
    peg_break_rew: 0.0

agent:
  # Core identification
  class: "PPO"
  disable_progressbar: true

  # Checkpoint and experiment tracking
  track_ckpts: true
  # ckpt_tracker_path comes from primary config automatically

  # SKRL PPO PARAMETERS (these override SKRL defaults)
  # rollouts and mini_batches are auto-calculated from episode timing
  learning_epochs: 4
  mini_batches: 150
  discount_factor: 0.99
  lambda_: 0.95  # Note: underscore for Python compatibility
  random_timesteps: 0
  learning_starts: 0
  random_value_timesteps: 150
  mixed_precision: false

  # CUSTOM LEARNING PARAMETERS (extensions to SKRL)
  policy_learning_rate: 1.0e-6
  critic_learning_rate: 1.0e-5
  value_update_ratio: 4 #2

  # Optimizer parameters
  optimizer_betas: [0.999, 0.999]
  optimizer_eps: 1.0e-8
  optimizer_weight_decay: 0

  # Loss and clipping
  use_huber_value_loss: true
  grad_norm_clip: 0.5
  ratio_clip: 0.2
  clip_predicted_values: false
  value_clip: 0.2
  entropy_loss_scale: 0.1
  value_loss_scale: 1.0
  kl_threshold: 0.05
  time_limit_bootstrap: true

  # Reward shaping
  reward_shaper_type: 'const_scale'
  rewards_shaper_scale: 0.1

  # Preprocessors
  state_preprocessor: true
  value_preprocessor: true