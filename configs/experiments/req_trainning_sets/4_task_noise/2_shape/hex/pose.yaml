# Hybrid Agent Control Experiment Configuration 

# Base configuration to inherit from
base_config: "configs/base/PiH_base.yaml"

primary:
  agents_per_break_force: 5
  num_envs_per_agent: 256
  break_forces: [10000]  # Break force conditions (-1 = unbreakable)

  #agents_per_break_force: 5 # Number of agents per break force condition
  #num_envs_per_agent: 256   # Environments per individual agent
  max_steps: 3000064 #25000192

# Model configuration for hybrid agent
model:
  use_hybrid_agent: false
wrappers:
    # Spawn height curriculum learning
  spawn_height_curriculum:
    # current default in line with IndustREAL baseline
    enabled: true
    progress_threshold: 0.50         # Increase min_height when success rate >= this
    regress_threshold: 0.10          # Decrease min_height when success rate < this
    progress_height_delta: 0.005     # Increase min by 5mm when succeeding
    regression_height_delta: 0.003   # Decrease min by 3mmm when struggling
    min_height: -0.011    

  force_reward:
    
    enable_contact_reward: false
    contact_reward_weight: 2.0
    
environment:
  ctrl: 
    pos_action_bounds: [0.05, 0.05, 0.08]
  task:
    hand_init_pos: [0.0, 0.0, 0.077]
  # OBSERVATION RANDOMIZATION CONFIGURATION
  obs_rand:
    fixed_asset_pos: [0.000, 0.000, 0.000]  
  
  task:
    asset_variant: hex_short_small
# Experiment metadata
experiment:
  name: &exp_name "pose_curr_noZcon_higherBounds_noPosNoise_hex"
  tags: ['exp_task_shape']
  group: *exp_name