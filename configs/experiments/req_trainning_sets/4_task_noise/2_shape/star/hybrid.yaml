# Hybrid Agent Control Experiment Configuration 

# Base configuration to inherit from
base_config: "configs/base/PiH_base.yaml"

primary:
  agents_per_break_force: 5 # Number of agents per break force condition
  num_envs_per_agent: 256   # Environments per individual agent
  max_steps: 7500064 #25000192

# Model configuration for hybrid agent
model:
  use_hybrid_agent: false
  hybrid_agent:
    force_init_std: 0.1 # implies a 96% range [-2.5, 2.5] 

# Enable hybrid control wrapper and related systems
wrappers:
  # Enable hybrid control wrapper
  hybrid_control:
    enabled: true
    reward_type: "none"
    use_ground_truth_selection: false
    
  pose_contact_logging:
    enabled: false

  # Custom keypoint offset patterns for non-default peg shapes
  keypoint_offset:
    enabled: true
    mode: 'polygon'  # 'axis' or 'polygon'
    # Axis mode parameters
    num_keypoints: 4
    x_scale: 0.015
    y_scale: 0.015
    z_scale: 0.15

    # Polygon mode parameters
    num_lines: 5
    num_keypoints_z: 4
    num_keypoints_radial: 4
    xy_scale: 0.015

  force_reward:
    
    enable_contact_reward: false
    contact_reward_weight: 2.0

agent:
  supervised_selection_loss_weight: 1.0
#  policy_learning_rate: 5.0e-4
#  critic_learning_rate: 1.0e-4
#  #learning_epochs: 1

environment:
  task:
    asset_variant: star_short_small
    hand_init_orn_noise: [0.0, 0.0, 0.5236]
  # OBSERVATION RANDOMIZATION CONFIGURATION
  obs_rand:
    fixed_asset_pos: [0.001, 0.001, 0.001]  
    
# Experiment metadata
experiment:
  name: &exp_name "basic-hybrid_star"
  tags: ['exp_task_shape']
  group: *exp_name