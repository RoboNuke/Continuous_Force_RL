# Hybrid Agent Control Experiment Configuration

# Base configuration to inherit from
base_config: "configs/base/PiH_base.yaml"

primary:
  agents_per_break_force: 5 # Number of agents per break force condition
  num_envs_per_agent: 256   # Environments per individual agent
  max_steps: 3000064 #25000192


environment:
  sim:
    physx:
      solver_type: 1
      max_position_iteration_count: &pos_iter 32  # Important to avoid interpenetration.
      max_velocity_iteration_count: &vel_iter 8
      bounce_threshold_velocity: 0.2
      friction_offset_threshold: 0.01
      friction_correlation_distance: 0.00625
      #gpu_max_rigid_contact_count: 2**23
      #gpu_max_rigid_patch_count: 2**23
      gpu_max_num_partitions: 1

  robot:
    spawn:
      rigid_props:
        solver_position_iteration_count: *pos_iter
        solver_velocity_iteration_count: *vel_iter
        linear_damping: &lin_damp 0.0

      articulation_props:
        solver_position_iteration_count: *pos_iter
        solver_velocity_iteration_count: *vel_iter

  task:
    fixed_asset:
      spawn:
        rigid_props:
          solver_position_iteration_count: *pos_iter
          solver_velocity_iteration_count: *vel_iter
          linear_damping: *lin_damp

    held_asset:
      spawn:
        rigid_props:
          solver_position_iteration_count: *pos_iter
          solver_velocity_iteration_count: *vel_iter
          linear_damping: *lin_damp

  ctrl:
    apply_ema_force: true
    async_z_force_bounds: true
    force_action_bounds: [20.0, 20.0, 20.0] #
    torque_action_bounds: [0.5, 0.5, 0.5]
    force_action_threshold: [2.0, 2.0, 2.0] #
    torque_action_threshold: [0.1, 0.1, 0.1]

    default_task_prop_gains: [565.0, 565.0, 565.0, 28.0, 28.0, 28.0]
    #default_task_prop_gains: [100, 100, 100, 30, 30, 30]
    default_task_force_gains: [0.5, 0.5, 0.5, 0.001, 0.001, 0.001]

    no_sel_ema: true
    use_delta_force: false  # false=absolute (action*bounds), true=delta (action*threshold+current)

    # Force PID control
    enable_force_derivative: false  # Enable D term (velocity damping) in force control
    enable_force_integral: false    # Enable I term in force control
    default_task_force_integ_gains: [0.01, 0.01, 0.01, 0.001, 0.001, 0.001]  # Ki gains
    force_integral_clamp: 50.0      # Anti-windup clamp for integral term

  # OBSERVATION RANDOMIZATION CONFIGURATION
  obs_rand:
    fixed_asset_pos: [0.001, 0.001, 0.001]  # 2.5mm default from FORGE

# Model configuration for hybrid agent
model:
  use_hybrid_agent: true
  hybrid_agent:
    force_init_std: 0.1

wrappers:
  # Fragile object behavior
  fragile_objects:
    enabled: true
    peg_break_rew: -15.0

  # Efficient environment resetting
  efficient_reset:
    enabled: true
    terminate_on_success: false  # If true, episodes terminate immediately upon success
    success_bonus: 1.0  # Total reward on success (base env gives +1, wrapper adjusts to this total)
    use_remaining_steps_bonus: false  # If true, bonus = max_episode_length - steps_taken (overrides success_bonus)

  # Factory-specific metrics tracking
  factory_metrics:
    enabled: true
    publish_to_wandb: true
    engagement_reward_scale: 60.0  # Scale for engagement rewards (default 1.0 = no change)
    success_reward_scale: 40.0
    timeout_penalty: -20.0 # Penalty applied on timeout (max steps reached, default 0.0 = no penalty)
    # num_agents is computed from primary config automatically

  force_reward:
    enabled: true

    enable_contact_reward: false
    contact_reward_weight: 2.0

    # relative to desired force configs
    ideal_force: 5.0

    enable_softplus: false
    softplus_weight: 1.0

    enable_abs_target: False
    abs_target_weight: 1.0

    enable_forge_style: False
    forge_style_weight: 1.0

  hybrid_control:
    enabled: true
    reward_type: "none"
    use_ground_truth_selection: false

  pose_contact_logging:
    enabled: false

agent:
  supervised_selection_loss_weight: 1.0

environment:
  # OBSERVATION RANDOMIZATION CONFIGURATION
  obs_rand:
    fixed_asset_pos: [0.001, 0.001, 0.001]

# Experiment metadata
experiment:
  name: &exp_name "LCLoP_stiff_newRew"
  tags: []
  group: *exp_name
