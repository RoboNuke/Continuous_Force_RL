# Hybrid Agent Control Experiment Configuration 

# Base configuration to inherit from
base_config: "configs/base/PiH_base.yaml"

primary:
  #agents_per_break_force: 1  # Number of agents per break force condition
  #num_envs_per_agent: 16    # Environments per individual agent
  ckpt_tracker_path: "/nfs/stak/users/brownhun/logFix_hybrid_agent_tracker.txt"
  break_forces: [25]

# Model configuration for hybrid agent
model:
  use_hybrid_agent: true
  last_layer_scale: 1.0
  hybrid_agent:
    unit_std_init: false
    init_bias: 2.5 #1.1
    init_scale_last_layer: false
    init_layer_scale: 1.0
    pre_layer_scale_factor: 1.0

    unit_factor_std_init: 1.0
    
  use_separate_heads: true
  selection_head_hidden_dim: 64
  component_head_hidden_dim: 128

# Enable hybrid control wrapper and related systems
wrappers:
  # Enable hybrid control wrapper
  hybrid_control:
    enabled: true
    reward_type: "simp"
    
  pose_contact_logging:
    enabled: false

environment:
  hybrid_task:
    good_force_cmd_rew: 0.134
    bad_force_cmd_rew: -0.134 #-0.4

agent:
  policy_learning_rate: 1.0e-5
  critic_learning_rate: 1.0e-5
  #learning_epochs: 1

# Experiment metadata
experiment:
  name: "noScale_bias_simp"
  tags: []
  group: "noScale_bias_simp"