# Hybrid Agent Control Experiment Configuration 

# Base configuration to inherit from
base_config: "configs/base/PiH_base.yaml"

primary:
  agents_per_break_force: 5 # Number of agents per break force condition
  num_envs_per_agent: 256   # Environments per individual agent
  max_steps: 729600 #3000064 #25000192

# Model configuration for hybrid agent
model:
  use_hybrid_agent: false
  use_separate_heads: false
  selection_head_hidden_dim: 2
  component_head_hidden_dim: 256
  
  # Actor architecture (SimBa policy network)
  actor:
    n: 2  # Number of SimBa layers
    latent_size: 512  # Hidden dimension

  # Critic architecture (SimBa value network)
  critic:
    n: 3  # Number of SimBa layers
    latent_size: 1024  # Hidden dimension

environment:
  # OBSERVATION RANDOMIZATION CONFIGURATION
  ctrl:
    default_task_prop_gains: [565.0, 565.0, 565.0, 28.0, 28.0, 28.0]
  obs_rand:
    fixed_asset_pos: [0.0025, 0.0025, 0.0025]  
   
agent:
  supervised_selection_loss_weight: 1.0

  learning_epochs: 4
  policy_learning_rate: 1.0e-5
  critic_learning_rate: 5.0e-5
  value_update_ratio: 2
  discount_factor: 0.99
  lambda_: 0.95  # Note: underscore for Python compatibility

# Experiment metadata
experiment:
  name: &exp_name "pose_modLR"
  tags: []
  group: *exp_name