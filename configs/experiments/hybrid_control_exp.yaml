# Hybrid Control Experiment Configuration
# Extends base factory configuration to enable hybrid force-position control

# Inherit from base configuration
base: "base/factory_base"

# Override primary parameters for hybrid control experiment
primary:
  agents_per_break_force: 2  # 2 agents per break force condition
  num_envs_per_agent: 256
  break_forces: -1  # Break force condition (-1 = unbreakable)
  episode_length_s: 15.0  # Longer episodes for complex control
  max_steps: 15360000  # More training steps for hybrid control

# Environment overrides for hybrid control
environment:
  episode_length_s: ${primary.episode_length_s}

# Model configuration for hybrid agent
model:
  use_hybrid_agent: true
  critic_output_init_mean: 60  # Higher initial value estimate for complex task

  # Hybrid agent specific parameters
  hybrid_agent:
    ctrl_torque: false  # Start with force-only control
    unit_std_init: true
    selection_adjustment_types: 'none'
    init_bias: -2.5  # Bias toward position control initially
    uniform_sampling_rate: 0.0

# Enable hybrid control wrapper and related systems
wrappers:
  # Enable force-torque sensor for hybrid control
  force_torque_sensor:
    enabled: true
    use_tanh_scaling: true
    tanh_scale: 0.05  # Slightly higher scaling for hybrid control

  # Enable hybrid control wrapper
  hybrid_control:
    enabled: true
    ctrl_torque: false  # Match model configuration
    reward_type: "simp"  # Simple reward structure to start

  # Enable observation noise for robustness
  observation_noise:
    enabled: true
    global_noise_scale: 0.8  # Slightly reduced noise for complex control
    apply_to_critic: false  # Clean critic observations for hybrid control
    noise_groups:
      fingertip_pos:
        std: 0.008  # Reduced noise for precise force control
      joint_pos:
        std: 0.004
      force_torque:  # Add force sensor noise
        noise_type: "gaussian"
        std: 0.02
        enabled: true
        timing: "step"

  # Enhanced action logging for hybrid control analysis
  action_logging:
    enabled: true
    track_action_histograms: true
    track_observation_histograms: false
    logging_frequency: 50  # More frequent logging for hybrid actions

# Learning configuration adjustments for hybrid control
learning:
  policy_learning_rate: 5.0e-7  # Lower learning rate for stable hybrid learning
  critic_learning_rate: 8.0e-6
  learning_epochs: 5  # More epochs for complex policy
  entropy_loss_scale: 0.001  # Small entropy bonus for exploration
  kl_threshold: 0.03  # Tighter KL constraint

# Experiment metadata
experiment:
  name: "hybrid_control_exp"
  tags: ["hybrid_control", "force_position", "factory"]
  group: "hybrid_control_experiments"
  wandb_project: "Continuous_Force_RL"
  wandb_entity: "hur"