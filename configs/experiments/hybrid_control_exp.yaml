# Hybrid Control Experiment Configuration
# Extends base factory configuration to enable hybrid force-position control

# Inherit from base configuration
base: "base/factory_base"

# Override primary parameters for hybrid control experiment
primary:
  agents_per_break_force: 2  # 2 agents per break force condition
  num_envs_per_agent: 256
  break_forces: -1  # Break force condition (-1 = unbreakable)
  max_steps: 15360000  # More training steps for hybrid control

# Environment overrides for hybrid control
environment:
  episode_length_s: ${primary.episode_length_s}

  # Control configuration (will be merged into Isaac Lab's ctrl config)
  ctrl:
    #pos_action_bounds: [0.05, 0.05, 0.05]
    force_action_bounds: [50.0, 50.0, 50.0]
    torque_action_bounds: [0.5, 0.5, 0.5]
    force_action_threshold: [10.0, 10.0, 10.0]
    torque_action_threshold: [0.1, 0.1, 0.1]
    default_task_force_gains: [0.1, 0.1, 0.1, 0.001, 0.001, 0.001]


# Model configuration for hybrid agent
model:
  use_hybrid_agent: true
  critic_output_init_mean: 60  # Higher initial value estimate for complex task

  # Hybrid agent specific parameters
  use_hybrid_agent: true
  hybrid_agent:
    ctrl_torque: ${primary.ctrl_torque}
    unit_std_init: true
    pos_init_std: 1.0
    rot_init_std: 1.0
    force_init_std: 1.0

    # init scale
    pos_scale: 1.0
    rot_scale: 1.0
    force_scale: 1.0
    torque_scale: 1.0

    # sel adjustments
    selection_adjustment_types: 'none'
    init_scale_weights_factor: 0.1
    init_bias: -2.5
    pre_layer_scale_factor: 0.1
    init_scale_last_layer: true
    init_layer_scale: 1.0
    uniform_sampling_rate: 0.0

# Enable hybrid control wrapper and related systems
wrappers:
  # Enable force-torque sensor for hybrid control
  force_torque_sensor:
    enabled: true
    use_tanh_scaling: false
    tanh_scale: 0.05  # Slightly higher scaling for hybrid control

  # Enable hybrid control wrapper
  hybrid_control:
    enabled: true
    ctrl_torque: false  # Match model configuration
    reward_type: "simp"  # Simple reward structure to start

  # Enable observation noise for robustness
  observation_noise:
    enabled: false
    global_noise_scale: 0.8  # Slightly reduced noise for complex control
    apply_to_critic: false  # Clean critic observations for hybrid control
    noise_groups:
      fingertip_pos:
        std: 0.008  # Reduced noise for precise force control
      joint_pos:
        std: 0.004
      force_torque:  # Add force sensor noise
        noise_type: "gaussian"
        std: 0.02
        enabled: true
        timing: "step"

  # Enhanced action logging for hybrid control analysis
  action_logging:
    enabled: false
    track_action_histograms: true
    track_observation_histograms: false
    logging_frequency: 50  # More frequent logging for hybrid actions

# Agent configuration adjustments for hybrid control
#agent:
#  policy_learning_rate: 5.0e-7  # Lower learning rate for stable hybrid learning
# Experiment metadata
experiment:
  name: "hybrid_control_exp"
  tags: ["hybrid_control", "force_position", "factory"]
  group: "hybrid_control_experiments"
  wandb_project: "Continuous_Force_RL"
  wandb_entity: "hur"