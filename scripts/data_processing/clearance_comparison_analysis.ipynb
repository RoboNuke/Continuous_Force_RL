{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clearance Comparison Analysis\n",
    "\n",
    "This notebook generates a 2Ã—N grid comparing performance across different clearance values:\n",
    "- Top row: Success Rate vs Position Noise\n",
    "- Bottom row: Break Rate vs Position Noise\n",
    "- Gold highlight box around the reference clearance level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 1: IMPORTS & CONSTANTS\n",
    "# ============================================================\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict\n",
    "\n",
    "# WandB Configuration\n",
    "ENTITY = \"hur\"\n",
    "PROJECT = \"SG_Exps\"\n",
    "\n",
    "# Clearance Levels - keys are internal names, each contains method tags\n",
    "CLEARANCE_LEVELS = {\n",
    "    \"0.5mm\": {\n",
    "        \"Pose\": \"PLACEHOLDER_POSE_0.5MM\",\n",
    "        \"Hybrid-Basic\": \"PLACEHOLDER_HYBRID_0.5MM\",\n",
    "        \"LCLoP\": \"PLACEHOLDER_LCLOP_0.5MM\",\n",
    "    },\n",
    "    \"1mm\": {\n",
    "        \"Pose\": \"PLACEHOLDER_POSE_1MM\",\n",
    "        \"Hybrid-Basic\": \"PLACEHOLDER_HYBRID_1MM\",\n",
    "        \"LCLoP\": \"PLACEHOLDER_LCLOP_1MM\",\n",
    "    },\n",
    "    \"2mm\": {\n",
    "        \"Pose\": \"PLACEHOLDER_POSE_2MM\",\n",
    "        \"Hybrid-Basic\": \"PLACEHOLDER_HYBRID_2MM\",\n",
    "        \"LCLoP\": \"PLACEHOLDER_LCLOP_2MM\",\n",
    "    },\n",
    "    \"4mm\": {\n",
    "        \"Pose\": \"PLACEHOLDER_POSE_4MM\",\n",
    "        \"Hybrid-Basic\": \"PLACEHOLDER_HYBRID_4MM\",\n",
    "        \"LCLoP\": \"PLACEHOLDER_LCLOP_4MM\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Display name mapping (for special cases if needed)\n",
    "CLEARANCE_DISPLAY_NAMES = {\n",
    "    \"0.5mm\": \"Clearance = 0.5mm\",\n",
    "    \"1mm\": \"Clearance = 1mm\",\n",
    "    \"2mm\": \"Clearance = 2mm\",\n",
    "    \"4mm\": \"Clearance = 4mm\",\n",
    "}\n",
    "\n",
    "# Highlight box - which clearance level to highlight with gold box\n",
    "HIGHLIGHT_CLEARANCE = \"1mm\"  # Set to None for no highlight\n",
    "\n",
    "# Evaluation Tags\n",
    "TAG_EVAL_PERFORMANCE = \"eval_performance\"\n",
    "TAG_EVAL_NOISE = \"eval_noise\"\n",
    "\n",
    "# Noise Level Mapping: display label -> metric range string\n",
    "NOISE_LEVELS = {\n",
    "    \"1mm\": \"0mm-1mm\",\n",
    "    \"2.5mm\": \"1mm-2.5mm\",\n",
    "    \"5mm\": \"2.5mm-5mm\",\n",
    "    \"7.5mm\": \"5mm-7.5mm\",\n",
    "}\n",
    "\n",
    "# Metrics\n",
    "METRIC_SUCCESS = \"num_successful_completions\"\n",
    "METRIC_BREAKS = \"num_breaks\"\n",
    "METRIC_TOTAL = \"total_episodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 2: DETERMINE BEST POLICY\n",
    "# ============================================================\n",
    "\n",
    "def get_best_checkpoint_per_run(api, method_tag):\n",
    "    \"\"\"Find the best checkpoint for each run with the given method tag.\"\"\"\n",
    "    runs = api.runs(\n",
    "        f\"{ENTITY}/{PROJECT}\",\n",
    "        filters={\"$and\": [{\"tags\": method_tag}, {\"tags\": TAG_EVAL_PERFORMANCE}]}\n",
    "    )\n",
    "    \n",
    "    best_checkpoints = {}\n",
    "    for run in runs:\n",
    "        history = run.history()\n",
    "        if history.empty:\n",
    "            print(f\"Warning: Run {run.name} has no history data\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate score: successes - breaks\n",
    "        history[\"score\"] = history[f\"Eval_Core/{METRIC_SUCCESS}\"] - history[f\"Eval_Core/{METRIC_BREAKS}\"]\n",
    "        best_idx = history[\"score\"].idxmax()\n",
    "        best_step = int(history.loc[best_idx, \"total_steps\"])\n",
    "        \n",
    "        best_checkpoints[run.id] = {\n",
    "            \"run_name\": run.name,\n",
    "            \"best_step\": best_step,\n",
    "            \"score\": history.loc[best_idx, \"score\"],\n",
    "        }\n",
    "        print(f\"    {run.name}: best checkpoint at step {best_step} (score: {history.loc[best_idx, 'score']:.0f})\")\n",
    "    \n",
    "    return best_checkpoints\n",
    "\n",
    "# Get best checkpoints for each clearance level and method\n",
    "api = wandb.Api()\n",
    "best_checkpoints = defaultdict(dict)  # best_checkpoints[clearance][method]\n",
    "\n",
    "for clearance, method_tags in CLEARANCE_LEVELS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Clearance: {clearance}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for method_name, method_tag in method_tags.items():\n",
    "        print(f\"\\n  {method_name} ({method_tag}):\")\n",
    "        best_checkpoints[clearance][method_name] = get_best_checkpoint_per_run(api, method_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 3: DOWNLOAD DATA\n",
    "# ============================================================\n",
    "\n",
    "def download_eval_noise_data(api, method_tag, best_checkpoints):\n",
    "    \"\"\"Download eval_noise data for best checkpoints across all noise levels.\"\"\"\n",
    "    runs = api.runs(\n",
    "        f\"{ENTITY}/{PROJECT}\",\n",
    "        filters={\"$and\": [{\"tags\": method_tag}, {\"tags\": TAG_EVAL_NOISE}]}\n",
    "    )\n",
    "\n",
    "    # Build lookup by agent number from best_checkpoints\n",
    "    checkpoint_by_agent = {}\n",
    "    for run_id, info in best_checkpoints.items():\n",
    "        agent_num = info[\"run_name\"].rsplit(\"_\", 1)[-1]\n",
    "        checkpoint_by_agent[agent_num] = info[\"best_step\"]\n",
    "\n",
    "    data = []\n",
    "    for run in runs:\n",
    "        # Extract agent number from run name\n",
    "        agent_num = run.name.rsplit(\"_\", 1)[-1]\n",
    "\n",
    "        if agent_num not in checkpoint_by_agent:\n",
    "            print(f\"Warning: No matching performance run for agent {agent_num} ({run.name})\")\n",
    "            continue\n",
    "\n",
    "        best_step = checkpoint_by_agent[agent_num]\n",
    "        history = run.history()\n",
    "        \n",
    "        if best_step not in history[\"total_steps\"].values:\n",
    "            print(f\"Warning: Checkpoint {best_step} not found in {run.name}\")\n",
    "            continue\n",
    "        \n",
    "        row = history[history[\"total_steps\"] == best_step].iloc[0]\n",
    "        \n",
    "        for noise_label, noise_range in NOISE_LEVELS.items():\n",
    "            prefix = f\"Noise_Eval({noise_range})_Core\"\n",
    "            data.append({\n",
    "                \"run_id\": run.id,\n",
    "                \"run_name\": run.name,\n",
    "                \"checkpoint\": best_step,\n",
    "                \"noise_level\": noise_label,\n",
    "                \"success\": row[f\"{prefix}/{METRIC_SUCCESS}\"],\n",
    "                \"breaks\": row[f\"{prefix}/{METRIC_BREAKS}\"],\n",
    "                \"total\": row[f\"{prefix}/{METRIC_TOTAL}\"],\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Download all data\n",
    "noise_data = defaultdict(dict)  # noise_data[clearance][method]\n",
    "\n",
    "for clearance, method_tags in CLEARANCE_LEVELS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Downloading data for Clearance: {clearance}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for method_name, method_tag in method_tags.items():\n",
    "        print(f\"\\n  {method_name}...\")\n",
    "        noise_data[clearance][method_name] = download_eval_noise_data(\n",
    "            api, method_tag, best_checkpoints[clearance][method_name]\n",
    "        )\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for clearance in CLEARANCE_LEVELS.keys():\n",
    "    print(f\"\\n{clearance}:\")\n",
    "    for method_name in CLEARANCE_LEVELS[clearance].keys():\n",
    "        df = noise_data[clearance][method_name]\n",
    "        if not df.empty:\n",
    "            num_runs = df[\"run_name\"].nunique()\n",
    "            print(f\"  {method_name}: {num_runs} runs\")\n",
    "        else:\n",
    "            print(f\"  {method_name}: No data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 4: CLEARANCE COMPARISON FIGURE\n",
    "# ============================================================\n",
    "\n",
    "# Policy Selection\n",
    "TOP_N_POLICIES = None  # Set to integer (e.g., 3) to use top N policies, or None for all\n",
    "\n",
    "# Highlight Configuration\n",
    "HIGHLIGHT_CLEARANCE_PLOT = \"1mm\"  # Which clearance to highlight with gold box, or None\n",
    "HIGHLIGHT_COLOR = \"gold\"\n",
    "HIGHLIGHT_LINEWIDTH = 3\n",
    "\n",
    "# N/A Configuration - for clearance levels where break rate is not applicable\n",
    "NA_CLEARANCES = []  # List of clearance keys to show N/A instead of break rate plot\n",
    "NA_TEXT = \"N/A\"  # Text to display in the N/A box\n",
    "\n",
    "# Figure Configuration\n",
    "FIGSIZE = (14, 6)  # Width x Height\n",
    "DPI = 150\n",
    "BAR_WIDTH = 0.25\n",
    "\n",
    "# Colors\n",
    "COLORS = {\n",
    "    \"Pose\": \"#2ca02c\",        # Green\n",
    "    \"Hybrid-Basic\": \"#ff7f0e\", # Orange\n",
    "    \"LCLoP\": \"#1f77b4\",       # Blue\n",
    "}\n",
    "\n",
    "# Font sizes\n",
    "FONT_SUPTITLE = 16\n",
    "FONT_TITLE = 12\n",
    "FONT_AXIS_LABEL = 10\n",
    "FONT_TICK = 9\n",
    "FONT_LEGEND = 9\n",
    "FONT_NA = 12  # Font size for N/A text\n",
    "\n",
    "# Axis configuration\n",
    "SUCCESS_Y_LIM = (0, 100)\n",
    "SUCCESS_Y_TICKS = [0, 20, 40, 60, 80, 100]\n",
    "BREAK_Y_LIM = (0, 25)\n",
    "BREAK_Y_TICKS = [0, 5, 10, 15, 20, 25]\n",
    "\n",
    "# Labels\n",
    "SUPTITLE = \"Performance vs Position Noise Across Clearance Values\"\n",
    "X_LABEL = \"Position Noise\"\n",
    "SUCCESS_Y_LABEL = \"Success Rate (%)\"\n",
    "BREAK_Y_LABEL = \"Break Rate (%)\"\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "def filter_top_n_runs(df, method_best_checkpoints, top_n):\n",
    "    \"\"\"Filter dataframe to only include top N runs by score.\"\"\"\n",
    "    if df.empty or top_n is None or len(method_best_checkpoints) <= top_n:\n",
    "        return df\n",
    "    sorted_runs = sorted(method_best_checkpoints.items(), key=lambda x: x[1][\"score\"], reverse=True)\n",
    "    top_run_names = {info[\"run_name\"] for _, info in sorted_runs[:top_n]}\n",
    "    # Match by agent number\n",
    "    top_agent_nums = {name.rsplit(\"_\", 1)[-1] for name in top_run_names}\n",
    "    return df[df[\"run_name\"].apply(lambda x: x.rsplit(\"_\", 1)[-1] in top_agent_nums)]\n",
    "\n",
    "def compute_rates(df, noise_labels, metric=\"success\"):\n",
    "    \"\"\"Compute success or break rates for each noise level. Returns zeros if df is empty.\"\"\"\n",
    "    if df.empty:\n",
    "        return [0] * len(noise_labels)\n",
    "    \n",
    "    rates = []\n",
    "    for noise_label in noise_labels:\n",
    "        subset = df[df[\"noise_level\"] == noise_label]\n",
    "        if not subset.empty:\n",
    "            total = subset[\"total\"].sum()\n",
    "            rate = 100 * subset[metric].sum() / total\n",
    "            rates.append(rate)\n",
    "        else:\n",
    "            rates.append(0)\n",
    "    return rates\n",
    "\n",
    "# Setup\n",
    "clearance_keys = list(CLEARANCE_LEVELS.keys())\n",
    "method_names = list(CLEARANCE_LEVELS[clearance_keys[0]].keys())\n",
    "noise_labels = list(NOISE_LEVELS.keys())\n",
    "n_clearances = len(clearance_keys)\n",
    "x = np.arange(len(noise_labels))\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(2, n_clearances, figsize=FIGSIZE, dpi=DPI)\n",
    "fig.suptitle(SUPTITLE, fontsize=FONT_SUPTITLE)\n",
    "\n",
    "# Plot each clearance level\n",
    "for col, clearance in enumerate(clearance_keys):\n",
    "    ax_success = axes[0, col]\n",
    "    ax_break = axes[1, col]\n",
    "    \n",
    "    display_name = CLEARANCE_DISPLAY_NAMES.get(clearance, clearance)\n",
    "    is_na_clearance = clearance in NA_CLEARANCES\n",
    "    \n",
    "    # Plot bars for each method (success rate)\n",
    "    for i, method_name in enumerate(method_names):\n",
    "        df = noise_data[clearance][method_name]\n",
    "        df = filter_top_n_runs(df, best_checkpoints[clearance][method_name], TOP_N_POLICIES)\n",
    "        \n",
    "        success_rates = compute_rates(df, noise_labels, \"success\")\n",
    "        offset = (i - len(method_names)/2 + 0.5) * BAR_WIDTH\n",
    "        \n",
    "        ax_success.bar(x + offset, success_rates, BAR_WIDTH, \n",
    "                       label=method_name, color=COLORS[method_name])\n",
    "        \n",
    "        # Only plot break rates if not an N/A clearance\n",
    "        if not is_na_clearance:\n",
    "            break_rates = compute_rates(df, noise_labels, \"breaks\")\n",
    "            ax_break.bar(x + offset, break_rates, BAR_WIDTH,\n",
    "                         label=method_name, color=COLORS[method_name])\n",
    "    \n",
    "    # Configure success rate subplot\n",
    "    ax_success.set_title(display_name, fontsize=FONT_TITLE)\n",
    "    ax_success.set_xticks(x)\n",
    "    ax_success.set_xticklabels(noise_labels, fontsize=FONT_TICK)\n",
    "    ax_success.set_ylim(SUCCESS_Y_LIM)\n",
    "    ax_success.set_yticks(SUCCESS_Y_TICKS)\n",
    "    ax_success.tick_params(axis='y', labelsize=FONT_TICK)\n",
    "    \n",
    "    # Configure break rate subplot\n",
    "    if is_na_clearance:\n",
    "        # Clear the axes and show N/A text\n",
    "        ax_break.set_xticks([])\n",
    "        ax_break.set_yticks([])\n",
    "        ax_break.text(0.5, 0.5, NA_TEXT, transform=ax_break.transAxes,\n",
    "                      fontsize=FONT_NA, ha='center', va='center',\n",
    "                      style='italic', color='gray')\n",
    "        # Keep the spines for visual consistency\n",
    "        for spine in ax_break.spines.values():\n",
    "            spine.set_visible(True)\n",
    "    else:\n",
    "        ax_break.set_xlabel(X_LABEL, fontsize=FONT_AXIS_LABEL)\n",
    "        ax_break.set_xticks(x)\n",
    "        ax_break.set_xticklabels(noise_labels, fontsize=FONT_TICK)\n",
    "        ax_break.set_ylim(BREAK_Y_LIM)\n",
    "        ax_break.set_yticks(BREAK_Y_TICKS)\n",
    "        ax_break.tick_params(axis='y', labelsize=FONT_TICK)\n",
    "    \n",
    "    # Only show y-axis label on leftmost plots\n",
    "    if col == 0:\n",
    "        ax_success.set_ylabel(SUCCESS_Y_LABEL, fontsize=FONT_AXIS_LABEL)\n",
    "        ax_break.set_ylabel(BREAK_Y_LABEL, fontsize=FONT_AXIS_LABEL)\n",
    "    \n",
    "    # Only show legend on first plot\n",
    "    if col == 0:\n",
    "        ax_success.legend(fontsize=FONT_LEGEND, loc='upper left')\n",
    "    \n",
    "    # Add gold highlight to spines if this is the highlighted clearance\n",
    "    if HIGHLIGHT_CLEARANCE_PLOT is not None and clearance == HIGHLIGHT_CLEARANCE_PLOT:\n",
    "        for spine in ['top', 'left', 'right', 'bottom']:\n",
    "            ax_success.spines[spine].set_color(HIGHLIGHT_COLOR)\n",
    "            ax_success.spines[spine].set_linewidth(HIGHLIGHT_LINEWIDTH)\n",
    "            ax_break.spines[spine].set_color(HIGHLIGHT_COLOR)\n",
    "            ax_break.spines[spine].set_linewidth(HIGHLIGHT_LINEWIDTH)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Leave room for suptitle\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
