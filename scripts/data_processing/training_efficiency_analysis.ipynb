{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Training Efficiency Analysis\n",
    "\n",
    "This notebook analyzes training efficiency by plotting success rate vs total steps:\n",
    "- Downloads training history from either training runs (Episode/success_rate) or eval runs\n",
    "- Plots mean success rate with 95% CI shaded region\n",
    "- Shows threshold line at 90% and labels steps to reach it\n",
    "\n",
    "Set `DATA_SOURCE = \"training\"` or `DATA_SOURCE = \"eval\"` in Block 1 to choose data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 1: IMPORTS & CONSTANTS\n",
    "# ============================================================\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# WandB Configuration\n",
    "ENTITY = \"hur\"\n",
    "PROJECT = \"SG_Exps\"\n",
    "\n",
    "# Method Tags\n",
    "METHOD_TAGS = {\n",
    "    \"Pose\": \"pose_perf-comp:2026-01-05_10:54\",\n",
    "    \"Hybrid-Basic\": \"basic-hybrid_perf-comp:2026-01-05_10:54\",\n",
    "    \"SWISH\": \"LCLoP_fix_perf-comp:2026-01-09_11:17\",\n",
    "}\n",
    "\n",
    "# Data Source Configuration\n",
    "# Set to \"training\" to use Episode/success_rate from training runs\n",
    "# Set to \"eval\" to use Eval_Core metrics from eval_performance runs\n",
    "DATA_SOURCE = \"training\"\n",
    "\n",
    "# Evaluation Tags (used when DATA_SOURCE = \"eval\")\n",
    "TAG_EVAL_PERFORMANCE = \"eval_performance\"\n",
    "\n",
    "# Metrics\n",
    "METRIC_SUCCESS_TRAINING = \"Episode/success_rate\"  # For training runs\n",
    "METRIC_SUCCESS_EVAL = \"num_successful_completions\"  # For eval runs\n",
    "METRIC_TOTAL_EVAL = \"total_episodes\"  # For eval runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrobonuke\u001b[0m (\u001b[33mhur\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Source: training\n",
      "============================================================\n",
      "\n",
      "Pose (pose_perf-comp:2026-01-05_10:54):\n",
      "  pose_perf-comp_f(10)_0: 78 checkpoints\n",
      "  pose_perf-comp_f(10)_1: 78 checkpoints\n",
      "  pose_perf-comp_f(10)_3: 78 checkpoints\n",
      "  pose_perf-comp_f(10)_2: 78 checkpoints\n",
      "  pose_perf-comp_f(10)_4: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_0: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_1: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_3: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_2: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_4: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_0: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_1: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_3: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_2: 78 checkpoints\n",
      "  Eval_noise_pose_perf-comp_f(10)_4: 78 checkpoints\n",
      "\n",
      "Hybrid-Basic (basic-hybrid_perf-comp:2026-01-05_10:54):\n",
      "  basic-hybrid_perf-comp_f(10)_0: 78 checkpoints\n",
      "  basic-hybrid_perf-comp_f(10)_1: 78 checkpoints\n",
      "  basic-hybrid_perf-comp_f(10)_2: 78 checkpoints\n",
      "  basic-hybrid_perf-comp_f(10)_3: 78 checkpoints\n",
      "  basic-hybrid_perf-comp_f(10)_4: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_1: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_0: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_2: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_3: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_4: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_1: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_0: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_2: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_3: 78 checkpoints\n",
      "  Eval_noise_basic-hybrid_perf-comp_f(10)_4: 78 checkpoints\n",
      "\n",
      "SWISH (LCLoP_fix_perf-comp:2026-01-09_11:17):\n",
      "  fPiH_base_f(10)_0: 78 checkpoints\n",
      "  fPiH_base_f(10)_1: 78 checkpoints\n",
      "  fPiH_base_f(10)_2: 78 checkpoints\n",
      "  fPiH_base_f(10)_3: 78 checkpoints\n",
      "  fPiH_base_f(10)_4: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_0: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_1: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_2: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_4: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_3: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_0: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_1: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_2: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_3: 78 checkpoints\n",
      "  Eval_noise_fPiH_base_f(10)_4: 78 checkpoints\n",
      "\n",
      "============================================================\n",
      "DATA SUMMARY\n",
      "============================================================\n",
      "Pose: 5 runs, 78 unique steps\n",
      "Hybrid-Basic: 5 runs, 78 unique steps\n",
      "SWISH: 5 runs, 78 unique steps\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 2: DOWNLOAD TRAINING DATA\n",
    "# ============================================================\n",
    "\n",
    "def download_training_history_from_training_runs(api, method_tag):\n",
    "    \"\"\"Download Episode/success_rate history from training runs (not eval runs).\"\"\"\n",
    "    # Get training runs (exclude eval_performance tag)\n",
    "    runs = api.runs(\n",
    "        f\"{ENTITY}/{PROJECT}\",\n",
    "        filters={\"$and\": [{\"tags\": method_tag}, {\"tags\": {\"$ne\": TAG_EVAL_PERFORMANCE}}]}\n",
    "    )\n",
    "    \n",
    "    all_data = []\n",
    "    for run in runs:\n",
    "        history = run.history()\n",
    "        if history.empty:\n",
    "            print(f\"Warning: Run {run.name} has no history data\")\n",
    "            continue\n",
    "        \n",
    "        # Extract relevant columns\n",
    "        for _, row in history.iterrows():\n",
    "            if pd.notna(row.get(\"total_steps\")) and pd.notna(row.get(METRIC_SUCCESS_TRAINING)):\n",
    "                # Episode/success_rate is already a percentage (0-100)\n",
    "                success_rate = row[METRIC_SUCCESS_TRAINING] * 100  # Convert from 0-1 to 0-100 if needed\n",
    "                all_data.append({\n",
    "                    \"run_id\": run.id,\n",
    "                    \"run_name\": run.name,\n",
    "                    \"total_steps\": int(row[\"total_steps\"]),\n",
    "                    \"success_rate\": success_rate,\n",
    "                })\n",
    "        \n",
    "        print(f\"  {run.name}: {len(history)} checkpoints\")\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def download_training_history_from_eval_runs(api, method_tag):\n",
    "    \"\"\"Download success rate history from eval_performance runs.\"\"\"\n",
    "    runs = api.runs(\n",
    "        f\"{ENTITY}/{PROJECT}\",\n",
    "        filters={\"$and\": [{\"tags\": method_tag}, {\"tags\": TAG_EVAL_PERFORMANCE}]}\n",
    "    )\n",
    "    \n",
    "    all_data = []\n",
    "    for run in runs:\n",
    "        history = run.history()\n",
    "        if history.empty:\n",
    "            print(f\"Warning: Run {run.name} has no history data\")\n",
    "            continue\n",
    "        \n",
    "        # Extract relevant columns\n",
    "        for _, row in history.iterrows():\n",
    "            if pd.notna(row.get(\"total_steps\")) and pd.notna(row.get(f\"Eval_Core/{METRIC_SUCCESS_EVAL}\")):\n",
    "                success_rate = 100 * row[f\"Eval_Core/{METRIC_SUCCESS_EVAL}\"] / row[f\"Eval_Core/{METRIC_TOTAL_EVAL}\"]\n",
    "                all_data.append({\n",
    "                    \"run_id\": run.id,\n",
    "                    \"run_name\": run.name,\n",
    "                    \"total_steps\": int(row[\"total_steps\"]),\n",
    "                    \"success_rate\": success_rate,\n",
    "                })\n",
    "        \n",
    "        print(f\"  {run.name}: {len(history)} checkpoints\")\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "# Download training data for each method\n",
    "api = wandb.Api()\n",
    "training_data = {}\n",
    "\n",
    "print(f\"Data Source: {DATA_SOURCE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for method_name, method_tag in METHOD_TAGS.items():\n",
    "    print(f\"\\n{method_name} ({method_tag}):\")\n",
    "    if DATA_SOURCE == \"training\":\n",
    "        training_data[method_name] = download_training_history_from_training_runs(api, method_tag)\n",
    "    else:\n",
    "        training_data[method_name] = download_training_history_from_eval_runs(api, method_tag)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "for method_name, df in training_data.items():\n",
    "    if not df.empty:\n",
    "        n_runs = df[\"run_name\"].nunique()\n",
    "        n_steps = df[\"total_steps\"].nunique()\n",
    "        print(f\"{method_name}: {n_runs} runs, {n_steps} unique steps\")\n",
    "    else:\n",
    "        print(f\"{method_name}: No data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# BLOCK 3: TRAINING EFFICIENCY PLOT\n# ============================================================\n\n# Figure Configuration\nFIGSIZE = (10, 6)\nDPI = 150\n\n# Colors\nCOLORS = {\n    \"Pose\": \"#2ca02c\",        # Green\n    \"Hybrid-Basic\": \"#ff7f0e\", # Orange\n    \"SWISH\": \"#1f77b4\",       # Blue\n}\n\n# Threshold Configuration\nSUCCESS_THRESHOLD = 80  # Percentage threshold for labeling\n# Shading alpha for confidence interval\nCI_ALPHA = 0.2\n\n# Font sizes\nFONT_TITLE = 14\nFONT_AXIS_LABEL = 12\nFONT_TICK = 10\nFONT_LEGEND = 10\nFONT_ANNOTATION = 9\n\n# Axis configuration\nY_LIM = (0, 100)\nY_TICKS = [0, 20, 40, 60, 80, 100]\n\n# Labels\nTITLE = \"Training Efficiency: Success Rate vs Training Steps\"\nX_LABEL = \"Total Steps\"\nY_LABEL = \"Success Rate (%)\"\n\n# Threshold line configuration\nTHRESHOLD_COLOR = \"black\"\nTHRESHOLD_LINESTYLE = \"--\"\nTHRESHOLD_LINEWIDTH = 1.5\n\n# Annotation toggle\nSHOW_ANNOTATIONS = True  # Set to False to hide threshold line and step annotations\n\n# Annotation configuration\nANNOTATION_COLOR = \"black\"\nANNOTATION_Y_POS = 5  # Y position above x-axis for text\nANNOTATION_Y_OFFSET = 6  # Offset for stacking annotations vertically\n\n# ============================================================\n\ndef compute_stats_by_step(df):\n    \"\"\"Compute mean and 95% CI for each unique step across all runs.\"\"\"\n    if df.empty:\n        return pd.DataFrame()\n    \n    # Group by total_steps and compute statistics\n    stats = df.groupby(\"total_steps\")[\"success_rate\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n    stats.columns = [\"total_steps\", \"mean\", \"std\", \"count\"]\n    \n    # Calculate SEM and 95% CI\n    stats[\"sem\"] = stats[\"std\"] / np.sqrt(stats[\"count\"])\n    stats[\"ci_95\"] = 1.96 * stats[\"sem\"]\n    \n    # Calculate upper and lower bounds (clipped to [0, 100])\n    stats[\"lower\"] = (stats[\"mean\"] - stats[\"ci_95\"]).clip(0, 100)\n    stats[\"upper\"] = (stats[\"mean\"] + stats[\"ci_95\"]).clip(0, 100)\n    \n    # Sort by steps\n    stats = stats.sort_values(\"total_steps\")\n    \n    return stats\n\ndef find_threshold_crossing(stats, threshold):\n    \"\"\"Find the first step where mean success rate >= threshold.\"\"\"\n    if stats.empty:\n        return None\n    \n    crossing = stats[stats[\"mean\"] >= threshold]\n    if crossing.empty:\n        return None\n    \n    return crossing.iloc[0][\"total_steps\"]\n\n# Create figure\nfig, ax = plt.subplots(figsize=FIGSIZE, dpi=DPI)\n\n# Store threshold crossings for annotations\nthreshold_crossings = {}\n\n# Plot each method\nfor method_name in METHOD_TAGS.keys():\n    df = training_data[method_name]\n    stats = compute_stats_by_step(df)\n    \n    if stats.empty:\n        print(f\"Warning: No data for {method_name}\")\n        continue\n    \n    color = COLORS[method_name]\n    \n    # Plot mean line\n    ax.plot(stats[\"total_steps\"], stats[\"mean\"], \n            color=color, label=method_name, linewidth=2)\n    \n    # Plot 95% CI shaded region\n    ax.fill_between(stats[\"total_steps\"], stats[\"lower\"], stats[\"upper\"],\n                    color=color, alpha=CI_ALPHA)\n    \n    # Find threshold crossing\n    crossing_step = find_threshold_crossing(stats, SUCCESS_THRESHOLD)\n    if crossing_step is not None:\n        threshold_crossings[method_name] = crossing_step\n\n# Plot threshold line (conditional on SHOW_ANNOTATIONS)\nif SHOW_ANNOTATIONS:\n    ax.axhline(y=SUCCESS_THRESHOLD, color=THRESHOLD_COLOR, \n               linestyle=THRESHOLD_LINESTYLE, linewidth=THRESHOLD_LINEWIDTH,\n               label=f\"{SUCCESS_THRESHOLD}% Threshold\")\n\n# Add threshold crossing annotations (conditional on SHOW_ANNOTATIONS)\nif SHOW_ANNOTATIONS:\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"STEPS TO REACH {SUCCESS_THRESHOLD}% SUCCESS RATE\")\n    print(\"=\" * 60)\n\n    for i, (method_name, crossing_step) in enumerate(sorted(threshold_crossings.items(), key=lambda x: x[1])):\n        # Print to console\n        print(f\"{method_name}: {crossing_step:,} steps\")\n        \n        # Add vertical line at crossing point\n        ax.axvline(x=crossing_step, color=ANNOTATION_COLOR, linestyle=\":\", alpha=0.5, linewidth=1)\n        \n        # Add text annotation above x-axis, next to vertical line (horizontal text)\n        y_pos = ANNOTATION_Y_POS + (i * ANNOTATION_Y_OFFSET)\n        ax.text(crossing_step, y_pos, f\"{method_name}: {crossing_step:,}\", \n                fontsize=FONT_ANNOTATION, color=ANNOTATION_COLOR,\n                ha='left', va='bottom')\n\n# Configure axes\nax.set_xlabel(X_LABEL, fontsize=FONT_AXIS_LABEL)\nax.set_ylabel(Y_LABEL, fontsize=FONT_AXIS_LABEL)\nax.set_title(TITLE, fontsize=FONT_TITLE)\nax.set_ylim(Y_LIM)\nax.set_yticks(Y_TICKS)\nax.tick_params(axis='both', labelsize=FONT_TICK)\n\n# Format x-axis with scientific notation or commas\nax.ticklabel_format(axis='x', style='scientific', scilimits=(6, 6))\n\n# Legend in bottom right\nax.legend(fontsize=FONT_LEGEND, loc='lower right')\n\n# Grid for readability\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaaclab_drail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}