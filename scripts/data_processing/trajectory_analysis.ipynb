{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Trajectory Analysis\n",
    "\n",
    "This notebook generates trajectory visualization plots for detailed analysis of policy behavior.\n",
    "\n",
    "**Features:**\n",
    "- Runs `wandb_eval.py` via subprocess if trajectory data doesn't exist\n",
    "- Multi-panel trajectory plots with phase annotations\n",
    "- Break analysis by phase\n",
    "- Selection confidence comparison\n",
    "- Selection probability trajectories by phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BLOCK 1: IMPORTS & CONSTANTS\n",
    "# ============================================================\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Rectangle\n",
    "import pickle\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "# WandB Configuration\n",
    "ENTITY = \"hur\"\n",
    "PROJECT = \"SG_Exps\"\n",
    "\n",
    "# Method Tags - internal name -> wandb tag\n",
    "METHOD_TAGS = {\n",
    "    #\"Pose(1mm)\": \"pose_task_frag:2026-01-06_00:52\",\n",
    "    #\"Pose(2.5mm)\" : \"pose_25mm-noise:2026-01-19_07:50_15N\",\n",
    "    #\"Pose-7.5mm\":\"pose_75mm-noise:2026-01-17_19:18\",\n",
    "    #\"MATCH(1mm)\": \"LCLoP_task_frag:2026-01-06_00:27\",\n",
    "    #\"Hybrid-Basic(1mm)\": \"basic-hybrid_task_frag:2026-01-06_00:56\",\n",
    "\n",
    "    # hex\n",
    "    #\"Pose\": \"pose_hex:2026-01-13_22:49\",\n",
    "    #\"SWISH\": \"LCLoP_hex:2026-01-13_22:49\",\n",
    "\n",
    "\n",
    "    #\"Pose(1mm)\": \"pose_base-case:2026-02-02_13:40\",\n",
    "    #\"Pose(2.5mm)\": \"pose_25mm:2026-02-02_13:40\",\n",
    "    #\"MATCH(1mm)\": \"MATCH_base-case:2026-02-04_17:46\",\n",
    "    #\"Hybrid-Basic(1mm)\": \"basic-hybrid_base-case:2026-02-02_13:40\",\n",
    "\n",
    "    #Pose(1mm)\": \"pose_breakable_f10N:2026-02-04_17:49\",\n",
    "    #\"Pose(2.5mm)\": \"pose_25mm_f10:2026-02-05_22:12\",\n",
    "    #\"MATCH(1mm)\": \"MATCH_breakable_10N:2026-02-04_17:49\",\n",
    "    #\"Hybrid-Basic(1mm)\": \"basic-hybrid_breakable_f10N:2026-02-04_17:49\",\n",
    "\n",
    "\n",
    "    #\"VIC\":  \"pose_vic_squashed_breakable_f10:2026-02-21_10:59\",\n",
    "    #\"Pose-1\": \"pose_squashed_breakable_f10:2026-02-21_10:59\",\n",
    "    \"MATCH\": \"MATCH_squashed_breakable_f10:2026-02-21_10:59\",\n",
    "    \"Hybrid\": \"hybrid_squashed_breakable_f10:2026-02-21_10:59\",\n",
    "}\n",
    "\n",
    "# Method display settings\n",
    "METHOD_COLORS = {\n",
    "    \"Pose(1mm)\": \"#2ca02c\",        # Green\n",
    "    \"Hybrid-Basic\": \"#ff7f0e\", # Orange\n",
    "    \"MATCH(1mm)\": \"#1f77b4\",       # Blue\n",
    "}\n",
    "\n",
    "# Evaluation Tags\n",
    "TAG_EVAL_PERFORMANCE = \"eval_performance\"\n",
    "\n",
    "# Metrics for best policy selection\n",
    "METRIC_SUCCESS = \"num_successful_completions\"\n",
    "METRIC_BREAKS = \"num_breaks\"\n",
    "\n",
    "# Trajectory Evaluation Configuration\n",
    "WANDB_EVAL_SCRIPT = \"../../eval/wandb_eval.py\"  # Relative to this notebook\n",
    "TRAJ_OUTPUT_BASE = \"../../eval/traj_data\"  # Base directory for trajectory data\n",
    "\n",
    "# Phase definitions (matching traj_vis.ipynb)\n",
    "PHASES = ['approaching', 'initial_contact', 'insertion']\n",
    "PHASE_COLORS = {\n",
    "    'approaching': '#90EE90',      # Light green\n",
    "    'initial_contact': '#FFD700',  # Gold\n",
    "    'insertion': '#87CEEB',        # Sky blue\n",
    "}\n",
    "\n",
    "# Outcome definitions\n",
    "OUTCOMES = ['success', 'break', 'timeout']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Method: MATCH (MATCH_squashed_breakable_f10:2026-02-21_10:59)\n",
      "============================================================\n",
      "  Found 10 training runs\n",
      "    MATCH_squashed_breakable_f(10)_0 (train:cfonbefc): best checkpoint at step 1497600 (score: 98)\n",
      "    MATCH_squashed_breakable_f(10)_3 (train:0ms1il9d): best checkpoint at step 2572800 (score: 98)\n",
      "    MATCH_squashed_breakable_f(10)_1 (train:ai09st92): best checkpoint at step 2726400 (score: 97)\n",
      "    MATCH_squashed_breakable_f(10)_2 (train:2xwwtxtt): best checkpoint at step 998400 (score: 94)\n",
      "    MATCH_squashed_breakable_f(10)_4 (train:e32ueab6): best checkpoint at step 998400 (score: 98)\n",
      "\n",
      "============================================================\n",
      "Method: Hybrid (hybrid_squashed_breakable_f10:2026-02-21_10:59)\n",
      "============================================================\n",
      "  Found 10 training runs\n",
      "    hybrid_squashed_breakable_f(10)_0 (train:q5qtlp6b): best checkpoint at step 2956800 (score: 99)\n",
      "    hybrid_squashed_breakable_f(10)_1 (train:brsf94pp): best checkpoint at step 1420800 (score: 98)\n",
      "    hybrid_squashed_breakable_f(10)_2 (train:gm0nq0dx): best checkpoint at step 2956800 (score: 96)\n",
      "    hybrid_squashed_breakable_f(10)_4 (train:2pll31ql): best checkpoint at step 2649600 (score: 99)\n",
      "    hybrid_squashed_breakable_f(10)_3 (train:wsh8hiyr): best checkpoint at step 2726400 (score: 97)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 2: DETERMINE BEST POLICY\n",
    "# ============================================================\n",
    "\n",
    "def get_best_checkpoint_per_run(api, method_tag):\n",
    "    \"\"\"\n",
    "    Find the best checkpoint for each training run based on eval_performance runs.\n",
    "    \n",
    "    Returns dict mapping training_run_id -> {run_name, best_step, score, eval_run_id}\n",
    "    \"\"\"\n",
    "    # Query eval runs\n",
    "    eval_runs = api.runs(\n",
    "        f\"{ENTITY}/{PROJECT}\",\n",
    "        filters={\"$and\": [{\"tags\": method_tag}, {\"tags\": TAG_EVAL_PERFORMANCE}]}\n",
    "    )\n",
    "    \n",
    "    # Query training runs (have method_tag but NOT eval_performance)\n",
    "    training_runs = api.runs(\n",
    "        f\"{ENTITY}/{PROJECT}\",\n",
    "        filters={\"$and\": [{\"tags\": method_tag}, {\"tags\": {\"$ne\": TAG_EVAL_PERFORMANCE}}]}\n",
    "    )\n",
    "    \n",
    "    # Build lookup: training run name -> training run id\n",
    "    training_run_lookup = {}\n",
    "    for run in training_runs:\n",
    "        training_run_lookup[run.name] = run.id\n",
    "    \n",
    "    print(f\"  Found {len(training_run_lookup)} training runs\")\n",
    "    \n",
    "    best_checkpoints = {}\n",
    "    for eval_run in eval_runs:\n",
    "        history = eval_run.history()\n",
    "        if history.empty:\n",
    "            print(f\"  Warning: Eval run {eval_run.name} has no history data\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate score: successes - breaks\n",
    "        history[\"score\"] = history[f\"Eval_Core/{METRIC_SUCCESS}\"] - history[f\"Eval_Core/{METRIC_BREAKS}\"]\n",
    "        best_idx = history[\"score\"].idxmax()\n",
    "        best_step = int(history.loc[best_idx, \"total_steps\"])\n",
    "        \n",
    "        # Extract training run name from eval run name\n",
    "        # Eval run: \"Eval_performance_pose_perf-comp_f(10)_0\"\n",
    "        # Training run: \"pose_perf-comp_f(10)_0\"\n",
    "        eval_run_name = eval_run.name\n",
    "        if eval_run_name.startswith(\"Eval_performance_\"):\n",
    "            training_run_name = eval_run_name[len(\"Eval_performance_\"):]\n",
    "        elif eval_run_name.startswith(\"Eval_noise_\"):\n",
    "            training_run_name = eval_run_name[len(\"Eval_noise_\"):]\n",
    "        else:\n",
    "            # Try to find a matching training run by suffix\n",
    "            training_run_name = eval_run_name\n",
    "        \n",
    "        # Look up training run ID\n",
    "        if training_run_name not in training_run_lookup:\n",
    "            print(f\"  Warning: Could not find training run for eval run {eval_run_name}\")\n",
    "            print(f\"    Expected training run name: {training_run_name}\")\n",
    "            continue\n",
    "        \n",
    "        training_run_id = training_run_lookup[training_run_name]\n",
    "        \n",
    "        best_checkpoints[training_run_id] = {\n",
    "            \"run_name\": training_run_name,\n",
    "            \"best_step\": best_step,\n",
    "            \"score\": history.loc[best_idx, \"score\"],\n",
    "            \"eval_run_id\": eval_run.id,\n",
    "            \"eval_run_name\": eval_run_name,\n",
    "        }\n",
    "        print(f\"    {training_run_name} (train:{training_run_id}): best checkpoint at step {best_step} (score: {history.loc[best_idx, 'score']:.0f})\")\n",
    "    \n",
    "    return best_checkpoints\n",
    "\n",
    "# Get best checkpoints for each method\n",
    "api = wandb.Api()\n",
    "best_checkpoints = {}  # best_checkpoints[method]\n",
    "\n",
    "for method_name, method_tag in METHOD_TAGS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Method: {method_name} ({method_tag})\")\n",
    "    print(f\"{'='*60}\")\n",
    "    best_checkpoints[method_name] = get_best_checkpoint_per_run(api, method_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/hunter/Continuous_Force_RL\n",
      "Eval script: /home/hunter/Continuous_Force_RL/eval/wandb_eval.py\n",
      "Output base: /home/hunter/Continuous_Force_RL/eval/traj_data\n",
      "\n",
      "============================================================\n",
      "Processing: MATCH\n",
      "============================================================\n",
      "\n",
      "  Run: MATCH_squashed_breakable_f(10)_0 (id: cfonbefc)\n",
      "    Best checkpoint: 1497600 (score: 98)\n",
      "    Data exists, skipping\n",
      "\n",
      "  Run: MATCH_squashed_breakable_f(10)_3 (id: 0ms1il9d)\n",
      "    Best checkpoint: 2572800 (score: 98)\n",
      "    Data exists, skipping\n",
      "\n",
      "  Run: MATCH_squashed_breakable_f(10)_1 (id: ai09st92)\n",
      "    Best checkpoint: 2726400 (score: 97)\n",
      "    Data exists, skipping\n",
      "\n",
      "  Run: MATCH_squashed_breakable_f(10)_2 (id: 2xwwtxtt)\n",
      "    Best checkpoint: 998400 (score: 94)\n",
      "    Data exists, skipping\n",
      "\n",
      "  Run: MATCH_squashed_breakable_f(10)_4 (id: e32ueab6)\n",
      "    Best checkpoint: 998400 (score: 98)\n",
      "    Data exists, skipping\n",
      "\n",
      "============================================================\n",
      "Processing: Hybrid\n",
      "============================================================\n",
      "\n",
      "  Run: hybrid_squashed_breakable_f(10)_0 (id: q5qtlp6b)\n",
      "    Best checkpoint: 2956800 (score: 99)\n",
      "    Data exists, skipping\n",
      "\n",
      "  Run: hybrid_squashed_breakable_f(10)_1 (id: brsf94pp)\n",
      "    Best checkpoint: 1420800 (score: 98)\n",
      "    Data exists, skipping\n",
      "\n",
      "  Run: hybrid_squashed_breakable_f(10)_2 (id: gm0nq0dx)\n",
      "    Best checkpoint: 2956800 (score: 96)\n",
      "    Data exists, skipping\n",
      "\n",
      "  Run: hybrid_squashed_breakable_f(10)_4 (id: 2pll31ql)\n",
      "    Best checkpoint: 2649600 (score: 99)\n",
      "    Data exists, skipping\n",
      "\n",
      "  Run: hybrid_squashed_breakable_f(10)_3 (id: wsh8hiyr)\n",
      "    Best checkpoint: 2726400 (score: 97)\n",
      "    Data exists, skipping\n",
      "\n",
      "============================================================\n",
      "TRAJECTORY DATA SUMMARY\n",
      "============================================================\n",
      "\n",
      "MATCH:\n",
      "  Path: /home/hunter/Continuous_Force_RL/eval/traj_data/MATCH_squashed_breakable_f10_2026-02-21_10_59\n",
      "    MATCH_squashed_breakable_f(10)_0: checkpoint 1497600 [OK]\n",
      "    MATCH_squashed_breakable_f(10)_3: checkpoint 2572800 [OK]\n",
      "    MATCH_squashed_breakable_f(10)_1: checkpoint 2726400 [OK]\n",
      "    MATCH_squashed_breakable_f(10)_2: checkpoint 998400 [OK]\n",
      "    MATCH_squashed_breakable_f(10)_4: checkpoint 998400 [OK]\n",
      "  Total: 5/5 runs have data\n",
      "\n",
      "Hybrid:\n",
      "  Path: /home/hunter/Continuous_Force_RL/eval/traj_data/hybrid_squashed_breakable_f10_2026-02-21_10_59\n",
      "    hybrid_squashed_breakable_f(10)_0: checkpoint 2956800 [OK]\n",
      "    hybrid_squashed_breakable_f(10)_1: checkpoint 1420800 [OK]\n",
      "    hybrid_squashed_breakable_f(10)_2: checkpoint 2956800 [OK]\n",
      "    hybrid_squashed_breakable_f(10)_4: checkpoint 2649600 [OK]\n",
      "    hybrid_squashed_breakable_f(10)_3: checkpoint 2726400 [OK]\n",
      "  Total: 5/5 runs have data\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 3: RUN TRAJECTORY EVALUATION (IF NEEDED)\n",
    "# ============================================================\n",
    "\n",
    "# Get absolute paths for the script and output directory\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, \"../..\"))\n",
    "WANDB_EVAL_SCRIPT_ABS = os.path.join(PROJECT_ROOT, \"eval/wandb_eval.py\")\n",
    "TRAJ_OUTPUT_BASE_ABS = os.path.join(PROJECT_ROOT, \"eval/traj_data\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Eval script: {WANDB_EVAL_SCRIPT_ABS}\")\n",
    "print(f\"Output base: {TRAJ_OUTPUT_BASE_ABS}\")\n",
    "\n",
    "\n",
    "def get_traj_output_path(method_tag: str) -> str:\n",
    "    \"\"\"Get the output directory for trajectory data for a method.\"\"\"\n",
    "    # Format: {base}/{tag}/\n",
    "    safe_tag = method_tag.replace(\":\", \"_\").replace(\"/\", \"_\")\n",
    "    return os.path.join(TRAJ_OUTPUT_BASE_ABS, safe_tag)\n",
    "\n",
    "\n",
    "def check_run_data_exists(output_dir: str, run_id: str, checkpoint: int) -> bool:\n",
    "    \"\"\"Check if trajectory data exists for a specific run and checkpoint.\"\"\"\n",
    "    run_dir = os.path.join(output_dir, run_id)\n",
    "    if not os.path.exists(run_dir):\n",
    "        return False\n",
    "    \n",
    "    # Check for the specific checkpoint pkl file\n",
    "    pkl_file = f\"traj_{checkpoint}.pkl\"\n",
    "    return os.path.exists(os.path.join(run_dir, pkl_file))\n",
    "\n",
    "\n",
    "def run_trajectory_eval(\n",
    "    method_tag: str,\n",
    "    checkpoint: int,\n",
    "    output_dir: str,\n",
    "    run_id: str,\n",
    "    dry_run: bool = False\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Run wandb_eval.py with trajectory mode for a specific run and checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        method_tag: WandB tag for the method\n",
    "        checkpoint: Checkpoint step to evaluate\n",
    "        output_dir: Directory to save trajectory data\n",
    "        run_id: Specific run ID to evaluate\n",
    "        dry_run: If True, print command without executing\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        sys.executable, WANDB_EVAL_SCRIPT_ABS,\n",
    "        \"--tag\", method_tag,\n",
    "        \"--checkpoint\", str(checkpoint),\n",
    "        \"--eval_mode\", \"trajectory\",\n",
    "        \"--traj_output_dir\", output_dir,\n",
    "        \"--entity\", ENTITY,\n",
    "        \"--project\", PROJECT,\n",
    "        \"--run_id\", run_id,\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nCommand: {' '.join(cmd)}\")\n",
    "    print(f\"Working directory: {PROJECT_ROOT}\")\n",
    "    \n",
    "    if dry_run:\n",
    "        print(\"[DRY RUN] Would execute above command\")\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        # Set up environment with PYTHONPATH including project root\n",
    "        env = os.environ.copy()\n",
    "        pythonpath = env.get('PYTHONPATH', '')\n",
    "        if pythonpath:\n",
    "            env['PYTHONPATH'] = f\"{PROJECT_ROOT}:{pythonpath}\"\n",
    "        else:\n",
    "            env['PYTHONPATH'] = PROJECT_ROOT\n",
    "        \n",
    "        print(f\"PYTHONPATH: {env['PYTHONPATH']}\")\n",
    "        \n",
    "        # Use Popen to stream output in real-time\n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        print(f\"Starting trajectory evaluation for {run_id} at checkpoint {checkpoint}...\")\n",
    "        print(\"-\"*60 + \"\\n\")\n",
    "        \n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True,\n",
    "            cwd=PROJECT_ROOT,\n",
    "            env=env,\n",
    "        )\n",
    "        \n",
    "        # Stream output line by line\n",
    "        output_lines = []\n",
    "        for line in process.stdout:\n",
    "            print(line, end='')\n",
    "            output_lines.append(line)\n",
    "        \n",
    "        # Wait for process to complete\n",
    "        return_code = process.wait()\n",
    "        \n",
    "        # Check for Python exceptions\n",
    "        output_text = ''.join(output_lines)\n",
    "        has_python_exception = 'Traceback (most recent call last):' in output_text\n",
    "        has_module_error = 'ModuleNotFoundError:' in output_text\n",
    "        \n",
    "        # Check for success indicators\n",
    "        has_success = 'Evaluation complete!' in output_text or 'Saved trajectory data' in output_text\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*60)\n",
    "        if return_code != 0:\n",
    "            print(f\"Trajectory evaluation FAILED with return code: {return_code}\")\n",
    "            return False\n",
    "        elif has_python_exception or has_module_error:\n",
    "            print(f\"Trajectory evaluation FAILED (Python exception detected)\")\n",
    "            return False\n",
    "        elif has_success:\n",
    "            print(\"Trajectory evaluation completed successfully!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"Trajectory evaluation completed (no explicit success/failure detected)\")\n",
    "            return True\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"Trajectory evaluation timed out after 1 hour\")\n",
    "        process.kill()\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Exception running trajectory eval: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "\n",
    "# Configuration for this run\n",
    "DRY_RUN = False  # Set to False to actually run evaluations\n",
    "FORCE_RERUN = False  # Set to True to re-run even if data exists\n",
    "\n",
    "# Check and run trajectory evaluations for each run at its own best checkpoint\n",
    "# Process ONE RUN AT A TIME to avoid multiple Isaac Sim instances\n",
    "traj_data_paths = {}  # traj_data_paths[method] = output_dir\n",
    "\n",
    "methods_to_process = list(METHOD_TAGS.items())\n",
    "evaluation_failed = False\n",
    "\n",
    "for method_name, method_tag in methods_to_process:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {method_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    method_checkpoints = best_checkpoints.get(method_name, {})\n",
    "    if not method_checkpoints:\n",
    "        print(f\"  No checkpoints found for {method_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Output directory is per-method (not per-checkpoint)\n",
    "    output_dir = get_traj_output_path(method_tag)\n",
    "    traj_data_paths[method_name] = output_dir\n",
    "    \n",
    "    # Process each run individually at its own best checkpoint\n",
    "    for run_id, run_info in method_checkpoints.items():\n",
    "        run_name = run_info[\"run_name\"]\n",
    "        best_step = run_info[\"best_step\"]\n",
    "        score = run_info[\"score\"]\n",
    "        \n",
    "        print(f\"\\n  Run: {run_name} (id: {run_id})\")\n",
    "        print(f\"    Best checkpoint: {best_step} (score: {score})\")\n",
    "        \n",
    "        # Check if data already exists for this run\n",
    "        if check_run_data_exists(output_dir, run_id, best_step) and not FORCE_RERUN:\n",
    "            print(f\"    Data exists, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    Running trajectory evaluation...\")\n",
    "        \n",
    "        success = run_trajectory_eval(\n",
    "            method_tag, best_step, output_dir, run_id, dry_run=DRY_RUN\n",
    "        )\n",
    "        \n",
    "        if not success and not DRY_RUN:\n",
    "            print(f\"\\n  WARNING: Evaluation failed for {run_name}\")\n",
    "            print(f\"  Stopping to avoid repeated failures.\")\n",
    "            evaluation_failed = True\n",
    "            break\n",
    "    \n",
    "    if evaluation_failed:\n",
    "        break\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAJECTORY DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for method_name, output_dir in traj_data_paths.items():\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Path: {output_dir}\")\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"  Status: MISSING (no data)\")\n",
    "        continue\n",
    "    \n",
    "    method_checkpoints = best_checkpoints.get(method_name, {})\n",
    "    total_runs = len(method_checkpoints)\n",
    "    found_runs = 0\n",
    "    \n",
    "    for run_id, run_info in method_checkpoints.items():\n",
    "        best_step = run_info[\"best_step\"]\n",
    "        exists = check_run_data_exists(output_dir, run_id, best_step)\n",
    "        status = \"OK\" if exists else \"MISSING\"\n",
    "        if exists:\n",
    "            found_runs += 1\n",
    "        print(f\"    {run_info['run_name']}: checkpoint {best_step} [{status}]\")\n",
    "    \n",
    "    print(f\"  Total: {found_runs}/{total_runs} runs have data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trajectory data...\n",
      "  Loaded MATCH: 5 runs, 500 episodes, 5 breaks\n",
      "  Loaded Hybrid: 5 runs, 500 episodes, 6 breaks\n",
      "\n",
      "Total: 1000 episodes, 11 break events\n",
      "\n",
      "Outcome breakdown:\n",
      "  Hybrid: success=483, break=6, timeout=11\n",
      "  MATCH: success=480, break=5, timeout=15\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BLOCK 4: DATA LOADING FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "def _standardize_step(step: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Ensure step dict has all expected fields with correct format.\n",
    "    Converts numpy arrays to lists, handles missing fields with defaults.\n",
    "    \"\"\"\n",
    "    field_map = {\n",
    "        'contact_force': ['contact_force', 'force', 'wrench'],\n",
    "        'contact_state': ['contact_state', 'in_contact', 'contact'],\n",
    "        'control_selection': ['control_selection', 'mode_selection', 'selection'],\n",
    "        'control_probability': ['control_probability', 'mode_prob', 'selection_prob'],\n",
    "        'velocity': ['velocity', 'ee_velocity', 'vel'],\n",
    "        'position_error': ['position_error', 'pos_error', 'pos_err'],\n",
    "        'force_error': ['force_error', 'f_error', 'force_err'],\n",
    "        'phase': ['phase'],\n",
    "        'peg_pos': ['peg_pos', 'ee_pos', 'position'],\n",
    "        'terminated': ['terminated', 'done', 'terminal'],\n",
    "        'rewards': ['rewards', 'reward'],\n",
    "    }\n",
    "    \n",
    "    def get_field(data, keys, default=None):\n",
    "        for k in keys:\n",
    "            if k in data:\n",
    "                val = data[k]\n",
    "                if hasattr(val, 'tolist'):\n",
    "                    return val.tolist()\n",
    "                return val\n",
    "        return default\n",
    "    \n",
    "    return {\n",
    "        'step': step.get('step', 0),\n",
    "        'phase': get_field(step, field_map['phase'], 'unknown'),\n",
    "        'contact_force': get_field(step, field_map['contact_force'], [0, 0, 0]),\n",
    "        'contact_state': get_field(step, field_map['contact_state'], False),\n",
    "        'control_selection': get_field(step, field_map['control_selection'], [0, 0, 0]),\n",
    "        'control_probability': get_field(step, field_map['control_probability'], [0.5, 0.5, 0.5]),\n",
    "        'velocity': get_field(step, field_map['velocity'], [0, 0, 0]),\n",
    "        'position_error': get_field(step, field_map['position_error'], [0, 0, 0]),\n",
    "        'force_error': get_field(step, field_map['force_error'], [0, 0, 0]),\n",
    "        'peg_pos': get_field(step, field_map['peg_pos'], None),\n",
    "        'terminated': get_field(step, field_map['terminated'], False),\n",
    "        'rewards': get_field(step, field_map['rewards'], {}),\n",
    "    }\n",
    "\n",
    "\n",
    "def _parse_episode(raw_ep: Dict, policy: str, episode_id: str, run_id: str) -> Tuple[Dict, Optional[Dict]]:\n",
    "    \"\"\"Parse a single episode into standardized format.\"\"\"\n",
    "    steps = raw_ep.get('policy_steps', raw_ep.get('steps', raw_ep.get('trajectory', [])))\n",
    "    \n",
    "    hole_pos = raw_ep.get('hole_pos', None)\n",
    "    initial_peg_pos = raw_ep.get('initial_peg_pos', None)\n",
    "    \n",
    "    if hole_pos is not None and hasattr(hole_pos, 'tolist'):\n",
    "        hole_pos = hole_pos.tolist()\n",
    "    if initial_peg_pos is not None and hasattr(initial_peg_pos, 'tolist'):\n",
    "        initial_peg_pos = initial_peg_pos.tolist()\n",
    "    \n",
    "    standardized_steps = []\n",
    "    for s in steps:\n",
    "        std_step = _standardize_step(s)\n",
    "        \n",
    "        # Compute insertion_depth and lateral_error if we have hole_pos and peg_pos\n",
    "        if hole_pos is not None and std_step.get('peg_pos') is not None:\n",
    "            peg_pos = std_step['peg_pos']\n",
    "            std_step['insertion_depth'] = hole_pos[2] - peg_pos[2]\n",
    "            # Lateral error = XY distance from peg to hole center\n",
    "            std_step['lateral_error'] = ((peg_pos[0] - hole_pos[0])**2 + (peg_pos[1] - hole_pos[1])**2)**0.5\n",
    "        \n",
    "        standardized_steps.append(std_step)\n",
    "    \n",
    "    break_sim_steps = raw_ep.get('break_sim_steps', None)\n",
    "    \n",
    "    # Infer outcome\n",
    "    outcome = raw_ep.get('outcome', None)\n",
    "    if outcome is None:\n",
    "        outcome = 'success' if any(s.get('terminated', False) for s in steps) else 'timeout'\n",
    "    \n",
    "    parsed_episode = {\n",
    "        'policy': policy,\n",
    "        'episode_id': episode_id,\n",
    "        'run_id': run_id,\n",
    "        'outcome': outcome,\n",
    "        'steps': standardized_steps,\n",
    "        'hole_pos': hole_pos,\n",
    "        'initial_peg_pos': initial_peg_pos,\n",
    "    }\n",
    "    \n",
    "    break_event = None\n",
    "    if outcome == 'break' and break_sim_steps is not None:\n",
    "        break_event = {\n",
    "            'policy': policy,\n",
    "            'episode_id': episode_id,\n",
    "            'run_id': run_id,\n",
    "            'sim_steps': [_standardize_step(s) for s in break_sim_steps]\n",
    "        }\n",
    "    \n",
    "    return parsed_episode, break_event\n",
    "\n",
    "\n",
    "def load_trajectory_data_for_method(\n",
    "    output_dir: str,\n",
    "    policy_name: str,\n",
    "    method_checkpoints: Dict[str, Dict],\n",
    "    verbose: bool = True\n",
    ") -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load trajectory data from .pkl files for a method.\n",
    "    \n",
    "    Each run has its own best checkpoint, so we load the specific pkl file\n",
    "    for each run based on its checkpoint.\n",
    "    \n",
    "    Args:\n",
    "        output_dir: Directory containing run subdirectories with .pkl files\n",
    "        policy_name: Name to assign to this policy\n",
    "        method_checkpoints: Dict mapping run_id -> {run_name, best_step, score}\n",
    "        verbose: Print loading progress\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (episode_data, break_events)\n",
    "    \"\"\"\n",
    "    output_dir = os.path.abspath(output_dir)\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        if verbose:\n",
    "            print(f\"Warning: Output directory does not exist: {output_dir}\")\n",
    "        return [], []\n",
    "    \n",
    "    episode_data = []\n",
    "    break_events = []\n",
    "    load_stats = {'runs': 0, 'episodes': 0, 'breaks': 0}\n",
    "    \n",
    "    for run_id, run_info in method_checkpoints.items():\n",
    "        best_step = run_info[\"best_step\"]\n",
    "        run_dir = os.path.join(output_dir, run_id)\n",
    "        pkl_file = f\"traj_{best_step}.pkl\"\n",
    "        pkl_path = os.path.join(run_dir, pkl_file)\n",
    "        \n",
    "        if not os.path.exists(pkl_path):\n",
    "            if verbose:\n",
    "                print(f\"  Warning: Missing {pkl_file} for run {run_id}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(pkl_path, 'rb') as f:\n",
    "                raw_data = pickle.load(f)\n",
    "            \n",
    "            # Parse the trajectory data\n",
    "            if isinstance(raw_data, dict) and 'trajectories' in raw_data:\n",
    "                trajectories = raw_data['trajectories']\n",
    "                for env_key, env_data in trajectories.items():\n",
    "                    env_idx = int(env_key.replace('env_', ''))\n",
    "                    episode_id = f\"{run_id}_env_{env_idx}\"\n",
    "                    parsed_ep, break_event = _parse_episode(env_data, policy_name, episode_id, run_id)\n",
    "                    episode_data.append(parsed_ep)\n",
    "                    load_stats['episodes'] += 1\n",
    "                    if break_event is not None:\n",
    "                        break_events.append(break_event)\n",
    "                        load_stats['breaks'] += 1\n",
    "            \n",
    "            load_stats['runs'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  Error loading {pkl_path}: {e}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Loaded {policy_name}: {load_stats['runs']} runs, {load_stats['episodes']} episodes, {load_stats['breaks']} breaks\")\n",
    "    \n",
    "    return episode_data, break_events\n",
    "\n",
    "\n",
    "# Load all trajectory data\n",
    "all_episode_data = []\n",
    "all_break_events = []\n",
    "\n",
    "print(\"Loading trajectory data...\")\n",
    "for method_name, output_dir in traj_data_paths.items():\n",
    "    method_checkpoints = best_checkpoints.get(method_name, {})\n",
    "    episodes, breaks = load_trajectory_data_for_method(\n",
    "        output_dir, method_name, method_checkpoints\n",
    "    )\n",
    "    all_episode_data.extend(episodes)\n",
    "    all_break_events.extend(breaks)\n",
    "\n",
    "print(f\"\\nTotal: {len(all_episode_data)} episodes, {len(all_break_events)} break events\")\n",
    "\n",
    "# Print outcome breakdown\n",
    "if all_episode_data:\n",
    "    outcome_counts = defaultdict(lambda: defaultdict(int))\n",
    "    for ep in all_episode_data:\n",
    "        outcome_counts[ep['policy']][ep['outcome']] += 1\n",
    "    \n",
    "    print(\"\\nOutcome breakdown:\")\n",
    "    for policy in sorted(outcome_counts.keys()):\n",
    "        counts = outcome_counts[policy]\n",
    "        print(f\"  {policy}: success={counts['success']}, break={counts['break']}, timeout={counts['timeout']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================\n# BLOCK 5: MULTI-PANEL TRAJECTORY PLOT (Phase-Based Normalization)\n# ============================================================\n#\n# Generates a 2x2 multi-panel figure with phase-based normalization:\n# - Each phase is normalized separately across all trajectories\n# - Phase proportions are computed from average phase lengths in the data\n# - Phase labels drawn directly on each phase region\n# - Each subplot has its own legend for methods\n\nimport importlib\nimport analysis_utils\nimportlib.reload(analysis_utils)\nfrom analysis_utils import (\n    # Font sizes\n    FONT_SUPTITLE, FONT_TITLE, FONT_AXIS_LABEL, FONT_TICK, FONT_LEGEND,\n    # Plot styling\n    DEFAULT_DPI, DEFAULT_FIGSIZE, TIGHT_PAD, TIGHT_W_PAD, TIGHT_H_PAD,\n    AXIS_LABEL_PAD_X, AXIS_LABEL_PAD_Y, TICK_PAD_X, TICK_PAD_Y,\n    LEGEND_HANDLE_LENGTH,\n    # Trajectory styling\n    TRAJ_PHASE_COLORS, TRAJ_PHASE_DISPLAY, TRAJ_PHASE_ALPHA,\n    SELECTION_PROB_COLORS,\n    TRAJ_PHASE_BOUNDARY_LINES, TRAJ_PHASE_BOUNDARY_COLOR,\n    TRAJ_PHASE_BOUNDARY_LINESTYLE, TRAJ_PHASE_BOUNDARY_LINEWIDTH,\n    TRAJ_PHASE_LABEL_FONTSIZE, TRAJ_PHASE_LABEL_ALPHA, TRAJ_PHASE_LABEL_Y_POS,\n)\n\n# Plot Configuration\nFIGSIZE = (12, 8)\nN_POINTS_TOTAL = 100  # Total points across all phases\nALPHA_FILL = 0.3  # Alpha for std fill\n\n# Phase configuration - order of phases\nPHASE_ORDER = ['approaching', 'initial_contact', 'insertion']\n\n# Method line styles\nMETHOD_STYLES = {\n    \"Pose(1mm)\": {\"linestyle\": \"--\", \"linewidth\": 2},\n    \"Hybrid-Basic\": {\"linestyle\": \"-.\", \"linewidth\": 2},\n    \"MATCH(1mm)\": {\"linestyle\": \"-\", \"linewidth\": 2},\n    \"Hybrid\": {\"linestyle\": \"-.\", \"linewidth\": 2},\n    \"MATCH\": {\"linestyle\": \"-\", \"linewidth\": 2},\n    \"LCLoP\": {\"linestyle\": \"-\", \"linewidth\": 2},\n}\n\n# Variables to plot - with subplot titles\n# 'negate': True to flip sign (for Z position and contact force)\n# 'type': 'standard' for normal plot, 'selection_prob' for special selection probability plot\n# 'policies': list of policies to include, or None for all\n# 'ylim': optional y-axis limits\nVARIABLES = [\n    {'key': 'insertion_depth', 'label': 'Z Position (mm)', 'title': 'End-Effector Z Trajectory', \n     'axis': None, 'scale': 1000, 'negate': True, 'type': 'standard'},\n    {'key': 'lateral_error', 'label': 'XY Error (mm)', 'title': 'Lateral Alignment Error', \n     'axis': None, 'scale': 1000, 'type': 'standard'},\n    {'key': 'control_probability', 'label': 'P(Force Control)', 'title': 'MATCH Force Control Selection Probability', \n     'type': 'selection_prob', 'policies': ['MATCH(1mm)', 'MATCH'], 'ylim': (0, 1)},\n    {'key': 'contact_force', 'label': 'Contact Force (N)', 'title': 'Contact Force Magnitude', \n     'axis': 2, 'scale': 1, 'negate': True, 'type': 'standard'},\n    {'key': 'control_selection', 'label': 'Force Control', 'title': 'MATCH Force Control Selection', \n     'type': 'force_selection', 'policies': ['MATCH(1mm)', 'MATCH'], 'ylim': (0, 1)},\n]\n\n# ============================================================\n\ndef extract_variable(step: Dict, var_config: Dict) -> Optional[float]:\n    \"\"\"Extract a single value from a step based on variable config.\"\"\"\n    key = var_config['key']\n    axis = var_config.get('axis')\n    scale = var_config.get('scale', 1)\n    negate = var_config.get('negate', False)\n    \n    val = step.get(key)\n    if val is None:\n        return None\n    \n    result = None\n    if axis == 'xy':\n        if isinstance(val, (list, np.ndarray)) and len(val) >= 2:\n            result = np.sqrt(val[0]**2 + val[1]**2) * scale\n    elif axis is not None:\n        if isinstance(val, (list, np.ndarray)) and len(val) > axis:\n            result = val[axis] * scale\n    else:\n        if isinstance(val, (list, np.ndarray)):\n            result = val[0] * scale if len(val) > 0 else None\n        else:\n            result = val * scale\n    \n    if result is not None and negate:\n        result = -result\n    \n    return result\n\n\ndef extract_selection_prob_z(step: Dict) -> Optional[float]:\n    \"\"\"Extract Z-axis selection probability.\"\"\"\n    val = step.get('control_probability')\n    if val is None:\n        return None\n    if isinstance(val, (list, np.ndarray)) and len(val) > 2:\n        return val[2]  # Z-axis\n    return None\n\n\ndef extract_selection_prob_xy(step: Dict) -> Optional[float]:\n    \"\"\"Extract average XY selection probability.\"\"\"\n    val = step.get('control_probability')\n    if val is None:\n        return None\n    if isinstance(val, (list, np.ndarray)) and len(val) >= 2:\n        return (val[0] + val[1]) / 2  # Average of X and Y\n    return None\n\n\ndef extract_force_selection_z(step: Dict) -> Optional[float]:\n    \"\"\"Extract Z-axis force control selection.\n    \n    Reads control_selection directly if available, otherwise thresholds control_probability.\n    \"\"\"\n    sel = step.get('control_selection')\n    if sel is not None and isinstance(sel, (list, np.ndarray)) and len(sel) > 2:\n        return float(sel[2])\n    val = step.get('control_probability')\n    if val is not None and isinstance(val, (list, np.ndarray)) and len(val) > 2:\n        return float(val[2] > 0.5)\n    return None\n\n\ndef extract_force_selection_xy(step: Dict) -> Optional[float]:\n    \"\"\"Extract average XY force control selection.\n    \n    Reads control_selection directly if available, otherwise thresholds control_probability.\n    \"\"\"\n    sel = step.get('control_selection')\n    if sel is not None and isinstance(sel, (list, np.ndarray)) and len(sel) >= 2:\n        return (float(sel[0]) + float(sel[1])) / 2\n    val = step.get('control_probability')\n    if val is not None and isinstance(val, (list, np.ndarray)) and len(val) >= 2:\n        return (float(val[0] > 0.5) + float(val[1] > 0.5)) / 2\n    return None\n\n\ndef compute_phase_proportions(episode_data: List[Dict]) -> Dict[str, float]:\n    \"\"\"\n    Compute phase proportions based on average phase lengths across all episodes.\n    \n    Returns:\n        Dict mapping phase name to proportion (sums to 1.0).\n    \"\"\"\n    phase_lengths = defaultdict(list)\n    \n    for episode in episode_data:\n        # Count steps per phase in this episode\n        phase_counts = defaultdict(int)\n        for step in episode['steps']:\n            phase = step.get('phase', 'unknown')\n            if phase in TRAJ_PHASE_COLORS:\n                phase_counts[phase] += 1\n        \n        # Record lengths for phases that exist in this episode\n        for phase in PHASE_ORDER:\n            if phase in phase_counts:\n                phase_lengths[phase].append(phase_counts[phase])\n    \n    # Compute average length for each phase\n    avg_lengths = {}\n    for phase in PHASE_ORDER:\n        if phase in phase_lengths and len(phase_lengths[phase]) > 0:\n            avg_lengths[phase] = np.mean(phase_lengths[phase])\n        else:\n            avg_lengths[phase] = 0\n    \n    # Convert to proportions\n    total_length = sum(avg_lengths.values())\n    if total_length == 0:\n        # Fallback to equal proportions\n        n_phases = len(PHASE_ORDER)\n        return {phase: 1.0 / n_phases for phase in PHASE_ORDER}\n    \n    proportions = {phase: length / total_length for phase, length in avg_lengths.items()}\n    \n    # Print computed proportions\n    print(\"Computed phase proportions from data:\")\n    for phase in PHASE_ORDER:\n        print(f\"  {phase}: {proportions[phase]:.1%} (avg {avg_lengths[phase]:.1f} steps)\")\n    \n    return proportions\n\n\ndef segment_episode_by_phase(episode: Dict) -> Dict[str, List[Dict]]:\n    \"\"\"\n    Segment an episode's steps by phase.\n    \n    Returns:\n        Dict mapping phase name to list of steps in that phase.\n    \"\"\"\n    segments = defaultdict(list)\n    \n    for step in episode['steps']:\n        phase = step.get('phase', 'unknown')\n        if phase in TRAJ_PHASE_COLORS:\n            segments[phase].append(step)\n    \n    return segments\n\n\ndef normalize_phase_segments(\n    episodes_by_policy: Dict[str, List[Dict]],\n    var_config: Dict,\n    phase_proportions: Dict[str, float],\n    n_points_total: int = 100\n) -> Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Normalize trajectories by phase and compute mean/std for each policy.\n    \n    Each phase is normalized to its allocated proportion of the total points.\n    \n    Returns:\n        Dict mapping policy name to (t, mean, std) arrays for the full normalized trajectory.\n    \"\"\"\n    # Calculate points per phase based on proportions\n    points_per_phase = {}\n    for phase in PHASE_ORDER:\n        points_per_phase[phase] = max(1, int(n_points_total * phase_proportions.get(phase, 0)))\n    \n    # Adjust to ensure total is exactly n_points_total\n    total_points = sum(points_per_phase.values())\n    diff = n_points_total - total_points\n    if diff != 0:\n        # Add/subtract from the largest phase\n        largest_phase = max(points_per_phase, key=points_per_phase.get)\n        points_per_phase[largest_phase] += diff\n    \n    # Build time axis\n    t_full = np.linspace(0, 1, n_points_total)\n    \n    results = {}\n    \n    for policy, episodes in episodes_by_policy.items():\n        all_trajectories = []\n        \n        for episode in episodes:\n            segments = segment_episode_by_phase(episode)\n            \n            # Build normalized trajectory for this episode\n            trajectory_parts = []\n            \n            for phase in PHASE_ORDER:\n                n_points = points_per_phase.get(phase, 0)\n                if n_points == 0:\n                    continue\n                \n                if phase in segments and len(segments[phase]) > 0:\n                    # Extract values for this phase\n                    values = []\n                    for step in segments[phase]:\n                        val = extract_variable(step, var_config)\n                        values.append(val if val is not None else np.nan)\n                    \n                    if len(values) > 1:\n                        # Interpolate to allocated points\n                        t_orig = np.linspace(0, 1, len(values))\n                        t_target = np.linspace(0, 1, n_points)\n                        interpolated = np.interp(t_target, t_orig, values)\n                        trajectory_parts.append(interpolated)\n                    elif len(values) == 1:\n                        # Single point - repeat\n                        trajectory_parts.append(np.full(n_points, values[0]))\n                    else:\n                        trajectory_parts.append(np.full(n_points, np.nan))\n                else:\n                    # Phase not present - fill with NaN\n                    trajectory_parts.append(np.full(n_points, np.nan))\n            \n            if trajectory_parts:\n                full_trajectory = np.concatenate(trajectory_parts)\n                if len(full_trajectory) == n_points_total:\n                    all_trajectories.append(full_trajectory)\n        \n        if all_trajectories:\n            trajectories = np.array(all_trajectories)\n            n = len(all_trajectories)\n            with np.errstate(all='ignore'):\n                mean = np.nanmean(trajectories, axis=0)\n                std = np.nanstd(trajectories, axis=0)\n            results[policy] = (t_full, mean, std, n)\n    \n    return results\n\n\ndef normalize_selection_prob_segments(\n    episodes_by_policy: Dict[str, List[Dict]],\n    extract_func,\n    phase_proportions: Dict[str, float],\n    n_points_total: int = 100\n) -> Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n    \"\"\"\n    Normalize selection probability trajectories by phase.\n    \n    Similar to normalize_phase_segments but uses a custom extraction function.\n    \"\"\"\n    # Calculate points per phase based on proportions\n    points_per_phase = {}\n    for phase in PHASE_ORDER:\n        points_per_phase[phase] = max(1, int(n_points_total * phase_proportions.get(phase, 0)))\n    \n    # Adjust to ensure total is exactly n_points_total\n    total_points = sum(points_per_phase.values())\n    diff = n_points_total - total_points\n    if diff != 0:\n        largest_phase = max(points_per_phase, key=points_per_phase.get)\n        points_per_phase[largest_phase] += diff\n    \n    # Build time axis\n    t_full = np.linspace(0, 1, n_points_total)\n    \n    results = {}\n    \n    for policy, episodes in episodes_by_policy.items():\n        all_trajectories = []\n        \n        for episode in episodes:\n            segments = segment_episode_by_phase(episode)\n            trajectory_parts = []\n            \n            for phase in PHASE_ORDER:\n                n_points = points_per_phase.get(phase, 0)\n                if n_points == 0:\n                    continue\n                \n                if phase in segments and len(segments[phase]) > 0:\n                    values = []\n                    for step in segments[phase]:\n                        val = extract_func(step)\n                        values.append(val if val is not None else np.nan)\n                    \n                    if len(values) > 1:\n                        t_orig = np.linspace(0, 1, len(values))\n                        t_target = np.linspace(0, 1, n_points)\n                        interpolated = np.interp(t_target, t_orig, values)\n                        trajectory_parts.append(interpolated)\n                    elif len(values) == 1:\n                        trajectory_parts.append(np.full(n_points, values[0]))\n                    else:\n                        trajectory_parts.append(np.full(n_points, np.nan))\n                else:\n                    trajectory_parts.append(np.full(n_points, np.nan))\n            \n            if trajectory_parts:\n                full_trajectory = np.concatenate(trajectory_parts)\n                if len(full_trajectory) == n_points_total:\n                    all_trajectories.append(full_trajectory)\n        \n        if all_trajectories:\n            trajectories = np.array(all_trajectories)\n            n = len(all_trajectories)\n            with np.errstate(all='ignore'):\n                mean = np.nanmean(trajectories, axis=0)\n                std = np.nanstd(trajectories, axis=0)\n            results[policy] = (t_full, mean, std, n)\n    \n    return results\n\n\ndef normalize_zoh_segments(\n    episodes_by_policy: Dict[str, List[Dict]],\n    extract_func,\n    phase_proportions: Dict[str, float],\n    n_points_total: int = 100\n) -> Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray, int]]:\n    \"\"\"\n    Normalize trajectories by phase using zero-order hold (ZOH) interpolation.\n    \n    Instead of linear interpolation, each target point takes the value of the\n    nearest-lower source point (sample-and-hold / floor interpolation).\n    \"\"\"\n    points_per_phase = {}\n    for phase in PHASE_ORDER:\n        points_per_phase[phase] = max(1, int(n_points_total * phase_proportions.get(phase, 0)))\n    \n    total_points = sum(points_per_phase.values())\n    diff = n_points_total - total_points\n    if diff != 0:\n        largest_phase = max(points_per_phase, key=points_per_phase.get)\n        points_per_phase[largest_phase] += diff\n    \n    t_full = np.linspace(0, 1, n_points_total)\n    \n    results = {}\n    \n    for policy, episodes in episodes_by_policy.items():\n        all_trajectories = []\n        \n        for episode in episodes:\n            segments = segment_episode_by_phase(episode)\n            trajectory_parts = []\n            \n            for phase in PHASE_ORDER:\n                n_points = points_per_phase.get(phase, 0)\n                if n_points == 0:\n                    continue\n                \n                if phase in segments and len(segments[phase]) > 0:\n                    values = []\n                    for step in segments[phase]:\n                        val = extract_func(step)\n                        values.append(val if val is not None else np.nan)\n                    \n                    if len(values) > 1:\n                        t_orig = np.linspace(0, 1, len(values))\n                        t_target = np.linspace(0, 1, n_points)\n                        indices = np.searchsorted(t_orig, t_target, side='right') - 1\n                        indices = np.clip(indices, 0, len(values) - 1)\n                        zoh = np.array(values)[indices]\n                        trajectory_parts.append(zoh)\n                    elif len(values) == 1:\n                        trajectory_parts.append(np.full(n_points, values[0]))\n                    else:\n                        trajectory_parts.append(np.full(n_points, np.nan))\n                else:\n                    trajectory_parts.append(np.full(n_points, np.nan))\n            \n            if trajectory_parts:\n                full_trajectory = np.concatenate(trajectory_parts)\n                if len(full_trajectory) == n_points_total:\n                    all_trajectories.append(full_trajectory)\n        \n        if all_trajectories:\n            trajectories = np.array(all_trajectories)\n            n = len(all_trajectories)\n            with np.errstate(all='ignore'):\n                mean = np.nanmean(trajectories, axis=0)\n                std = np.nanstd(trajectories, axis=0)\n            results[policy] = (t_full, mean, std, n)\n    \n    return results\n\n\ndef get_phase_boundaries(phase_proportions: Dict[str, float]) -> List[Tuple[float, float, str]]:\n    \"\"\"\n    Get phase boundaries as (start, end, phase_name) tuples in normalized coordinates.\n    \"\"\"\n    boundaries = []\n    cumulative = 0.0\n    \n    for phase in PHASE_ORDER:\n        proportion = phase_proportions.get(phase, 0)\n        if proportion > 0:\n            start = cumulative\n            end = cumulative + proportion\n            boundaries.append((start, end, phase))\n            cumulative = end\n    \n    return boundaries\n\n\ndef draw_phase_decorations(ax, phase_boundaries, draw_boundary_lines=TRAJ_PHASE_BOUNDARY_LINES):\n    \"\"\"\n    Draw phase background shading, labels, and optional boundary lines on an axes.\n    \"\"\"\n    for start, end, phase in phase_boundaries:\n        color = TRAJ_PHASE_COLORS.get(phase, '#cccccc')\n        display = TRAJ_PHASE_DISPLAY.get(phase, phase)\n\n        # Background shading\n        ax.axvspan(start, end, alpha=TRAJ_PHASE_ALPHA, color=color, zorder=0)\n\n        # Phase label centered in region\n        center_x = (start + end) / 2\n        ax.text(\n            center_x, TRAJ_PHASE_LABEL_Y_POS, display,\n            transform=ax.get_xaxis_transform(),\n            ha='center', va='top',\n            fontsize=TRAJ_PHASE_LABEL_FONTSIZE,\n            alpha=TRAJ_PHASE_LABEL_ALPHA,\n            fontstyle='italic',\n        )\n\n    # Vertical boundary lines between phases\n    if draw_boundary_lines:\n        for i, (start, end, phase) in enumerate(phase_boundaries):\n            if i > 0:  # Skip first boundary (left edge of plot)\n                ax.axvline(\n                    x=start,\n                    color=TRAJ_PHASE_BOUNDARY_COLOR,\n                    linestyle=TRAJ_PHASE_BOUNDARY_LINESTYLE,\n                    linewidth=TRAJ_PHASE_BOUNDARY_LINEWIDTH,\n                    zorder=2,\n                )\n\n\ndef plot_trajectory_multipanel(\n    episode_data: List[Dict],\n    variables: List[Dict],\n    n_points: int = 100,\n    figsize: Tuple[float, float] = (12, 8),\n    draw_boundary_lines: bool = TRAJ_PHASE_BOUNDARY_LINES,\n) -> plt.Figure:\n    \"\"\"\n    Create 2x2 multi-panel trajectory plot with phase-based normalization.\n    \"\"\"\n    n_vars = len(variables)\n    n_rows = 2\n    n_cols = 2\n    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize, dpi=DEFAULT_DPI)\n    axes_flat = axes.flatten()\n    \n    # Compute phase proportions from data\n    phase_proportions = compute_phase_proportions(episode_data)\n    \n    # Group episodes by policy\n    episodes_by_policy = defaultdict(list)\n    for ep in episode_data:\n        episodes_by_policy[ep['policy']].append(ep)\n    \n    # Get phase boundaries\n    phase_boundaries = get_phase_boundaries(phase_proportions)\n    \n    for ax_idx, var_config in enumerate(variables):\n        if ax_idx >= len(axes_flat):\n            break\n        ax = axes_flat[ax_idx]\n        \n        # Draw phase backgrounds, labels, and boundary lines\n        draw_phase_decorations(ax, phase_boundaries, draw_boundary_lines)\n        \n        plot_type = var_config.get('type', 'standard')\n        allowed_policies = var_config.get('policies', None)\n        \n        if plot_type == 'selection_prob':\n            filtered_episodes = {p: eps for p, eps in episodes_by_policy.items() \n                               if allowed_policies is None or p in allowed_policies}\n            \n            z_data = normalize_selection_prob_segments(filtered_episodes, extract_selection_prob_z, \n                                                        phase_proportions, n_points)\n            for policy, (t, mean, std, n) in z_data.items():\n                ci = 1.96 * std / np.sqrt(n)\n                ax.plot(t, mean, color=SELECTION_PROB_COLORS['z'], linestyle='-', \n                       linewidth=2, label='Z-axis')\n                ax.fill_between(t, mean - ci, mean + ci, color=SELECTION_PROB_COLORS['z'], \n                               alpha=ALPHA_FILL)\n            \n            xy_data = normalize_selection_prob_segments(filtered_episodes, extract_selection_prob_xy,\n                                                         phase_proportions, n_points)\n            for policy, (t, mean, std, n) in xy_data.items():\n                ci = 1.96 * std / np.sqrt(n)\n                ax.plot(t, mean, color=SELECTION_PROB_COLORS['xy'], linestyle='--', \n                       linewidth=2, label='XY-axes')\n                ax.fill_between(t, mean - ci, mean + ci, color=SELECTION_PROB_COLORS['xy'], \n                               alpha=ALPHA_FILL)\n        elif plot_type == 'force_selection':\n            filtered_episodes = {p: eps for p, eps in episodes_by_policy.items() \n                               if allowed_policies is None or p in allowed_policies}\n            \n            z_data = normalize_zoh_segments(filtered_episodes, extract_force_selection_z, \n                                            phase_proportions, n_points)\n            for policy, (t, mean, std, n) in z_data.items():\n                ci = 1.96 * std / np.sqrt(n)\n                ax.plot(t, mean, color=SELECTION_PROB_COLORS['z'], linestyle='-', \n                       linewidth=2, label='Z-axis')\n                ax.fill_between(t, mean - ci, mean + ci, color=SELECTION_PROB_COLORS['z'], \n                               alpha=ALPHA_FILL)\n            \n            xy_data = normalize_zoh_segments(filtered_episodes, extract_force_selection_xy,\n                                             phase_proportions, n_points)\n            for policy, (t, mean, std, n) in xy_data.items():\n                ci = 1.96 * std / np.sqrt(n)\n                ax.plot(t, mean, color=SELECTION_PROB_COLORS['xy'], linestyle='--', \n                       linewidth=2, label='XY-axes')\n                ax.fill_between(t, mean - ci, mean + ci, color=SELECTION_PROB_COLORS['xy'], \n                               alpha=ALPHA_FILL)\n        else:\n            policy_data = normalize_phase_segments(episodes_by_policy, var_config, \n                                                   phase_proportions, n_points)\n            \n            if allowed_policies is not None:\n                policy_data = {p: data for p, data in policy_data.items() if p in allowed_policies}\n            \n            for policy, (t, mean, std, n) in policy_data.items():\n                ci = 1.96 * std / np.sqrt(n)\n                color = METHOD_COLORS.get(policy, 'gray')\n                style = METHOD_STYLES.get(policy, {\"linestyle\": \"-\", \"linewidth\": 2})\n                \n                ax.plot(t, mean, color=color, label=policy, **style)\n                ax.fill_between(t, mean - ci, mean + ci, color=color, alpha=ALPHA_FILL)\n        \n        # Configure subplot\n        ax.set_title(var_config.get('title', var_config['label']), fontsize=FONT_TITLE)\n        ax.set_ylabel(var_config['label'], fontsize=FONT_AXIS_LABEL, labelpad=AXIS_LABEL_PAD_Y)\n        ax.tick_params(axis='x', labelsize=FONT_TICK, pad=TICK_PAD_X)\n        ax.tick_params(axis='y', labelsize=FONT_TICK, pad=TICK_PAD_Y)\n        ax.grid(True, alpha=0.3)\n        ax.set_xlim(0, 1)\n        \n        if 'ylim' in var_config:\n            ax.set_ylim(var_config['ylim'])\n        \n        # X-axis label on bottom row only\n        row = ax_idx // n_cols\n        if row == n_rows - 1:\n            ax.set_xlabel('Normalized Time', fontsize=FONT_AXIS_LABEL, labelpad=AXIS_LABEL_PAD_X)\n        \n        # Per-subplot legend for methods\n        ax.legend(fontsize=FONT_LEGEND, loc='best', handlelength=LEGEND_HANDLE_LENGTH)\n    \n    # Hide unused subplots\n    for idx in range(n_vars, len(axes_flat)):\n        axes_flat[idx].set_visible(False)\n    \n    plt.tight_layout(pad=TIGHT_PAD, w_pad=TIGHT_W_PAD, h_pad=TIGHT_H_PAD)\n    return fig\n\n# Generate the plot\nif all_episode_data:\n    fig = plot_trajectory_multipanel(\n        all_episode_data,\n        VARIABLES,\n        n_points=N_POINTS_TOTAL,\n        figsize=FIGSIZE\n    )\n    plt.show()\nelse:\n    print(\"No episode data loaded. Please run trajectory evaluation first.\")\n\n\n\ndef plot_trajectory_single(\n    episode_data: List[Dict],\n    var_config: Dict,\n    phase_proportions: Dict[str, float],\n    n_points: int = 100,\n    figsize: Tuple[float, float] = DEFAULT_FIGSIZE,\n    draw_boundary_lines: bool = TRAJ_PHASE_BOUNDARY_LINES,\n) -> plt.Figure:\n    \"\"\"\n    Create a single trajectory plot for one variable with phase-based normalization.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=figsize, dpi=DEFAULT_DPI)\n    \n    # Group episodes by policy\n    episodes_by_policy = defaultdict(list)\n    for ep in episode_data:\n        episodes_by_policy[ep['policy']].append(ep)\n    \n    # Get phase boundaries\n    phase_boundaries = get_phase_boundaries(phase_proportions)\n    \n    # Draw phase backgrounds, labels, and boundary lines\n    draw_phase_decorations(ax, phase_boundaries, draw_boundary_lines)\n    \n    plot_type = var_config.get('type', 'standard')\n    allowed_policies = var_config.get('policies', None)\n    \n    if plot_type == 'selection_prob':\n        filtered_episodes = {p: eps for p, eps in episodes_by_policy.items() \n                           if allowed_policies is None or p in allowed_policies}\n        \n        z_data = normalize_selection_prob_segments(filtered_episodes, extract_selection_prob_z, \n                                                    phase_proportions, n_points)\n        for policy, (t, mean, std, n) in z_data.items():\n            ci = 1.96 * std / np.sqrt(n)\n            ax.plot(t, mean, color=SELECTION_PROB_COLORS['z'], linestyle='-', \n                   linewidth=2, label='Z-axis')\n            ax.fill_between(t, mean - ci, mean + ci, color=SELECTION_PROB_COLORS['z'], \n                           alpha=ALPHA_FILL)\n        \n        xy_data = normalize_selection_prob_segments(filtered_episodes, extract_selection_prob_xy,\n                                                     phase_proportions, n_points)\n        for policy, (t, mean, std, n) in xy_data.items():\n            ci = 1.96 * std / np.sqrt(n)\n            ax.plot(t, mean, color=SELECTION_PROB_COLORS['xy'], linestyle='--', \n                   linewidth=2, label='XY-axes')\n            ax.fill_between(t, mean - ci, mean + ci, color=SELECTION_PROB_COLORS['xy'], \n                           alpha=ALPHA_FILL)\n    elif plot_type == 'force_selection':\n        filtered_episodes = {p: eps for p, eps in episodes_by_policy.items() \n                           if allowed_policies is None or p in allowed_policies}\n        \n        z_data = normalize_zoh_segments(filtered_episodes, extract_force_selection_z, \n                                        phase_proportions, n_points)\n        for policy, (t, mean, std, n) in z_data.items():\n            ci = 1.96 * std / np.sqrt(n)\n            ax.plot(t, mean, color=SELECTION_PROB_COLORS['z'], linestyle='-', \n                   linewidth=2, label='Z-axis')\n            ax.fill_between(t, mean - ci, mean + ci, color=SELECTION_PROB_COLORS['z'], \n                           alpha=ALPHA_FILL)\n        \n        xy_data = normalize_zoh_segments(filtered_episodes, extract_force_selection_xy,\n                                         phase_proportions, n_points)\n        for policy, (t, mean, std, n) in xy_data.items():\n            ci = 1.96 * std / np.sqrt(n)\n            ax.plot(t, mean, color=SELECTION_PROB_COLORS['xy'], linestyle='--', \n                   linewidth=2, label='XY-axes')\n            ax.fill_between(t, mean - ci, mean + ci, color=SELECTION_PROB_COLORS['xy'], \n                           alpha=ALPHA_FILL)\n    else:\n        policy_data = normalize_phase_segments(episodes_by_policy, var_config, \n                                               phase_proportions, n_points)\n        \n        if allowed_policies is not None:\n            policy_data = {p: data for p, data in policy_data.items() if p in allowed_policies}\n        \n        for policy, (t, mean, std, n) in policy_data.items():\n            ci = 1.96 * std / np.sqrt(n)\n            color = METHOD_COLORS.get(policy, 'gray')\n            style = METHOD_STYLES.get(policy, {\"linestyle\": \"-\", \"linewidth\": 2})\n            \n            ax.plot(t, mean, color=color, label=policy, **style)\n            ax.fill_between(t, mean - ci, mean + ci, color=color, alpha=ALPHA_FILL)\n    \n    # Configure plot\n\n    ax.set_ylabel(var_config['label'], fontsize=FONT_AXIS_LABEL, labelpad=AXIS_LABEL_PAD_Y)\n    ax.set_xlabel('Normalized Time', fontsize=FONT_AXIS_LABEL, labelpad=AXIS_LABEL_PAD_X)\n    ax.tick_params(axis='x', labelsize=FONT_TICK, pad=TICK_PAD_X)\n    ax.tick_params(axis='y', labelsize=FONT_TICK, pad=TICK_PAD_Y)\n    ax.grid(True, alpha=0.3)\n    ax.set_xlim(0, 1)\n    \n    if 'ylim' in var_config:\n        ax.set_ylim(var_config['ylim'])\n    \n    # Methods legend (no phase legend - phases are labeled directly)\n    ax.legend(fontsize=FONT_LEGEND, loc='best', handlelength=LEGEND_HANDLE_LENGTH)\n    \n    plt.tight_layout(pad=TIGHT_PAD, w_pad=TIGHT_W_PAD, h_pad=TIGHT_H_PAD)\n    return fig\n\n\n# Generate individual plots for each variable\nFIGSIZE = (3.5, 1.75)\nif all_episode_data:\n    # Compute phase proportions once for all plots\n    phase_proportions_single = compute_phase_proportions(all_episode_data)\n    \n    for var_config in VARIABLES:\n        fig = plot_trajectory_single(\n            all_episode_data,\n            var_config,\n            phase_proportions_single,\n            n_points=N_POINTS_TOTAL,\n            figsize=FIGSIZE\n        )\n        plt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaaclab_drail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}