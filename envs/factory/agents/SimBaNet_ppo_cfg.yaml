#seed: -1
#num_envs: 256
#break_force: -1
#replicate_physics: False
#scene:
#  replicate_physics: False
# this is the parameters for the BroNet shared model
models:
  force_encoding: None
  last_layer_scale: 0.1
  act_init_std: 1.0 # note this is paired with tanh on action mean and scaled by env ##0.025 # max linear motion is 0.05 => 99.6 will be in that range
  critic_output_init_mean: 20 #was tried # seemed to be early value policy drives to
  critic:
    n: 2
    latent_size: 1024
  actor:
    n: 1
    latent_size: 256 # default should be 128 but it doesn't feel right to be so small

agent:
  class: PPO
  rollouts: 150 # set by env
  learning_epochs: 4
  mini_batches: 150 #128 #steps / minibatch (256 envs * 150 steps per update)
  discount_factor: 0.99
  lambda: 0.99
  learning_rate: 1.0e-6

  ##### LR Kwargs #####
  #KLAdaptive 
  learning_rate_scheduler: KLAdaptiveLR #LinearWarmup  #KLAdaptiveLR  
  klAdaptive_lr_scheduler_kwargs:
    kl_threshold: 0.01
    min_lr: 1.0e-9
    #max_lr: 1.0e-3
    
  #LinearWarmup
  linear_warmup_learning_rate_scheduler_kwargs:
     start_factor: 1.0e-7
     end_factor: 1.0
     total_iters: 80
  #####################


  # preprocessor
  state_preprocessor: True
  value_preprocessor: True

  # below are not args
  random_timesteps: 0
  learning_starts: 0
  mixed_precision: False

  bias_selection: True
  grad_norm_clip: 0.5
  ratio_clip: 0.2
  clip_predicted_values: False
  value_clip: 0.2

  entropy_loss_scale: 0.0 #5.0e-3 #
  value_loss_scale: 1.0 #1.5
  kl_threshold: 0.05
  rewards_shaper_scale: 0.1
  time_limit_bootstrap: True

  
  hybrid_agent:
    # general params
    ctrl_torque: False

    # intial std for hybrid agent
    unit_std_init: True # if true hybrid_agent init stds are calculated and below is not used
    pos_init_std: 1.0 #0.118 #0.25
    rot_init_std: 1.0 #0.118
    force_init_std: 1.0 #0.118

    # scaling for hybrid agent
    pos_scale: 1.0
    rot_scale: 1.0
    force_scale: 1.0
    torque_scale: 1.0

    # Selection Bias Parameters
    selection_adjustment_types: 'none'
    init_scale_weights_factor: 0.1
    init_bias: -2.5  # 2.5=>7.58% force control rate
    pre_layer_scale_factor: 0.1
    
    init_scale_last_layer: True
    init_layer_scale: 0.1
    
    uniform_sampling_rate: 0.0
  agent_is_list: False
  disable_progressbar: True
  
  # logging and checkpoint
  track_layernorms: False
  track_input: False
  track_advantage_hists: False
  track_gradients: False
  track_action_hists: False
  
  experiment:
    directory: "DEFAULT_DIRECTORY" # pass arg to change this!
    experiment_name: "DEFAULT_EXP_NAME" # pass arg to change this
    write_interval: 150 #75 # set by env
    checkpoint_interval: 1500 # set by env
    project: "OVERRIDE_WITH_ARG" 
    tags: []
    group: ""
  

  track_ckpts: True
  ckpt_tracker_path: "/nfs/stak/users/brownhun/ckpt_tracker2.txt"
video_tracking:
  # note to use this must run with --video --enable_cameras
  record_training: True
  train_video_interval: 450
  record_evals: True
  video_length: 50


#defaults:
#  - override hydra/job_logging: custom
